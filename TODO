I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.2.0-pre2 (unstable)...................................
x refactoring: sometimes, the BtreeNodeProxy updates node->count after an
    operation; sometimes, this is handled in the Btree*Action class -> unify
- db_remote.cc has an evil cast which creates (currently suppressed) valgrind
    errors, fix this (i.e. by introducing a RemoteCursor, LocalCursor)!
      Cursor *c = new Cursor((LocalDatabase *)this); // TODO this cast is evil!!
o btree stores fixed-length records in the leaf
    also support record length of 0 ("key exists" vs "key does not exist")
o PAX layout: if record size is fix then do not store 1 byte flag for each key
o completely rewrite handling of extended keys
o ham_bench uses /opt/share/dict/words as data input
o introduce COMM version
o COMM: btree can compress keys (strings: prefix, everything: dict)
o COMM: use SIMD for fixed-length scans

o btree stores fixed-length records in the leaf
    x this size must be persistent
    x also support record length of 0 ("key exists" vs "key does not exist")
    x return HAM_INV_RECORD_SIZE in insert/insert_cursor
    x if page is large enough then store the record in the leaf, even
        if it's > 8 bytes (add persistent flags to force this behavior)
    x needs to move BtreeIndex::read_record into BtreeNodeProxy
    x clean up the whole flow, the BtreeNodeProxy and the iterators 
    x PAX: wrap record access with RecordProxy policy, no functional changes
    x PAX: support inline fixed length records
        x only for leafs! internal nodes have a fixed record length of 8
        x add method test_get_classname() for the BtreeNodeProxy
        x move all key-type tests to a btree fixture
        o verify that the btree-flags are persistent
        o needs new unittests for leaf nodes/root nodes
        o needs new unittests for internal nodes
    o PAX: if record size is fix then do not store 1 byte flag for each key
        o also for non-inline fixed length records (> 32 byte)
    o extend unittests, ham_bench, ham_info, ham_dump, samples, monster tests,
        valgrind tests, perftests, documentation, tutorial
        x verify that the btree-flags are persistent
        x needs new unittests for leaf nodes/root nodes
        x needs new unittests for internal nodes
        x currently, records are limited by a constant ("32")
            x replace the magic number with an enum
            x ignore limit if the FORCE flag was set manually
            x add a unittest
    x extend ham_info
    x extend samples
    x extend ham_export, ham_import
    x extend ham_bench
        x specify record size (support "0"!)
        x force inline records
    x add support for java API
    x add support for dotnet API
    x extend monster tests
    x extend valgrind tests
    x extend perftests
    x document that this setting is OPTIONAL but RECOMMENDED! (same for
        HAM_PARAM_KEY_TYPE)
    o PAX: if record size is fix then do not store 1 byte flag for each key
        o also for non-inline fixed length records (> 32 byte)

x clean up PageManager::allocate_page and node initialization;
    PageManager has parameter "page-type", and depending on the type
    the page is initialized. No further initialization is required
    in the btree or anywhere else.

o completely rewrite handling of extended keys
    If the extended key does not fit into 1/10th of the pagesize: allocate
    an overflow area, move the FULL key into that area. no more prefix
    compare. compare functions no longer need to return errors. get rid of
    the extkey-cache.
    o create a new page layout for variable keys (non-PAX)
    o use linear search
    o when merging/shifting make sure that the pages actually fit; minkey/maxkey
        does not make much sense with variable keys
    o monster.pl needs to create variable length extended keys for testing!
    o add (non-persistent) skiplist index
        o built on request
        o add every Nth element to the index (i.e. once per 2kb)
        o update on insert and erase
        o destroy after shift/merge
    o also use linear search with fixed length keys (if max-keys-per-page
        or the subrange in the binary search is low enough, and if the
        performance makes sense)
    o support fixed-length records
    o count number of overflow areas for statistics!

o introduce COMM version
    o closed repository
    o one source base with different licensing headers, different licensing
        string, different version tag
    o "hooks" for git to add changes without introducing conflicts
    o API to get licensee information (already there)
    o new release process
    o prebuilt win32 files for each customer
    o extra documentation
    o webpage updates, PR, mailings
    o file format interoperability?

o ham_bench improvements
    o have more data sources based on dict/words (concatenate, if there are
        not enough or if they are too short)
      o StringRandomSource(max)
      o StringAscendingSource(max)
      o StringDescendingSource(max)
      o StringZipfianSource(max)

o COMM: btree can compress keys
    o try to reduce the changes to a new KeyProxy object
    o get rid of the whole minkey/maxkey handling because these numbers
        are irrelevant with compression
    o dictionary-compression for every data type
        o dict is in-page, persistent
    o prefix-compression for strings
        o each 2kb have a full string (indexed by skiplist)

. COMM: use SIMD for fixed-length scans
    http://pcl.intel-research.net/publications/palm.pdf

. BtreeCursor: use memory arena for uncoupling the key
    -> better wait till extended keys are gone

. remove libjson, use boost::property_tree instead!
    o also on Windows!

o extend the documentation
    o HAM_PARAM_RECORD_SIZE
        o tutorial (also add info about HAM_PARAM_KEY_TYPE, HAM_PARAM_KEY_SIZE)
        o use ham_info to see if the record is inline
        o update "performance" wiki
        o update "ham_bench" wiki

---------------------- release 2.1.4 (unstable) -----------------------------

high-level plan for 2.2.0 (stable)...........................................
- completely rewrite handling of duplicate keys
    - replaces the legacy btree layout
    - store record list in leaf, not in overflow area; if there are too
        many then allocate an overflow extend
    - support inline records
    - move the whole record handling from the proxy down to the layout
- lay the groundwork for concurrency -> LSS-streams, finalize the file format
    (every buffer needs stream descriptor, trailer)
- cache-oblivious page distribution? (maybe write the LSS first?)
- compress the whole LSS
- hot backups -> COMM
- first COMM release

high-level plan for 2.2.1 ...................................................
- a transaction update is very similar to a delta update
- both have to be consolidated
- btree can handle delta updates
    - (or postpone till concurrency?)
    - delta updates are merged when there are too many of them, or if a cursor
        traverses the page (only if memcpy/memmove takes too much time)

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. it would be interesting to know the distribution of changelog "sizes" during
    a test (i.e. how many operations result in changelogs of size 1, 2, 3 etc)
    and how many operations actually require the WAL

. review/rewrite/refactor Transaction class
    keep in mind that sooner or later the BtreeNode will expect template
    arguments; can we do something similar with the TransactionNode?
    x try to get the methods/design analoguous to the Btree - no, that
        does not make too much sense
    x document the tree structure in the header file
    x refactor the code
    o split into multiple files
    o try to cleanup the flow; move more code from db.cc to txn.cc
    . each Transaction should have its own PoolAllocator (based on ByteArray);
        when deleting the whole structure, only the PoolAllocator is freed
        o only do this if memory allocations cost performance
        o problem: a realloc() will not always work because it might move
            the allocated memory, and all existing pointers would be invalid 

. also remove locking from C# and Java APIs


------------------- idea soup ---------------------------------------------

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o flush transactions in background (when the btree is concurrent)

o Improve leaf pages caching
    Store start/end key of each leaf page in a separate lookup table in order
    to avoid btree traversals. This could be part of the hinter.
  - one such cache per database
  - should work for insert/find/erase

o allow transactions w/o journal

o allow transactions w/o recovery

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o when flushing the Changeset: batch ALL changes for the WHOLE transaction,
    then flush all of them together. This way we can "merge" multiple changes
    for the same page.
    Also review the whole flush process - when not to log etc.
    - only 1 page affected: no need to log it because it is idempotent
    - freelist pages are always idempotent
    - more than 1 index page? not idempotent (most likely)
    - more than 1 blob page? not idempotent (maybe)
    o define a few benchmarks
    o be careful: if N operations are modifying the same changelog, and
        then #N+1 aborts then the aborting operation must NOT clear the
        changelog!

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o changeset: instead of simply adding pages to the changeset, the caller
    could already specify whether this page needs logging or not;
    i.e. after freelist rewrite, the blob pages do not need logging if a
    blob is deleted  

o is there a way to group all changeset flushes of a Transaction into one
    changeset, and batch-commit multiple commits? that way we would avoid the
    frequent syncs and performance would be improved
    o would have to remove all of assert(changeset.is_empty())
    o but we can use that assert prior to txn_begin

o BtreeFindAction: always use a cursor, and when doing approx matching
    then simply move left or right with that cursor

o flush in background (asynchronously)
    o need new flag file HAM_DISABLE_ASYNCHRONOUS_FLUSH
    o if in-memory database: disable async flush
    o if transactions are disabled: disable async flush
    o if enabled: create background thread, wait for signal
    o ham_env_flush: if txn are enabled then try to flush them to disk
    o how to deal with an error in the background thread???
        o store in Environment, then return in every exported function
    o default: async flush is OFF!

    o extend monster tests
        o with async flush
        o without async flush
        o extend/run performance test
        o run monster tests

    o documentation
        o tutorial
        o faq

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    -> or: make this the default; call the new flag HAM_TXN_MAYBE_WILL_ABORT
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o ham_get_count: could be atomically updated with every journal entry

=======
>>>>>>> Updated TODO
