I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.1.6 ..................................................
x improve static code analysis
x erlang api
o improved logging/journalling
o simplified btree SMOs
    paper:
        describe current solution
        describe new solution
            split SMOs from actual operation
            show degenerated tree
            show SMO operations (which can run in background)
        show before/after benchmarks
o delta updates


x clang's scan-build ignores ham_assert macros
    http://clang-analyzer.llvm.org/annotations.html#custom_assertions
    x also for coverity

x improved logging/journalling
    x change journal magic
    x add trailer for each entry (type + size)
    x need to keep track of the newest lsn (is currently written to the
        log file header)
    x files must be buffered
        x flush buffers if a txn is committed (fsync!)
        x if the buffer becomes too big
        x if the file is closed (no - just clear the buffer!)
    x merge both files (log + journal) into one
        x add new journal entry for "changeset"
        x then write the full changeset to the journal
        x flush buffers after the changeset was written (fsync)
        x remove log.h, log.cc
        x os_writev still required?
    x recovery processes physical log "backwards" and logical journal
        "forwards"
        x first go backwards; but if there's no trailer magic then skip the
            changeset
        x when going forwards: skip ALL changesets
        x ... and start with lsn of the applied changeset
    x still has to work if journalling is disabled but recovery is enabled
    x check recovery tests
    x collect metric for bytes flushed to disk
        x add to ham_bench
    x changeset flush - always log if > 1 page is flushed
    x add documentation

x erlang API
    x with quickcheck coverage for...
        x database operations (create/close/open/erase/rename)
        x database operations with insert/erase/find

o PageManager state now writes deltas to journal while flushing a txn-op.
    o the deltas are part of the Changeset
    o the deltas have to be idempotent
    o the deltas must be applied during recovery of the changeset
    o check recovery tests

. web-page requires updates
    x deployed html differs from git-repository
    o www1-repository and hamsterdb-www should be identical
    o www1
        o merge git repository of www1 (host on github, keep remote branch)
        o clean up 'dl' directory
        o where to host static files?
        o backup and deploy to www2, use round-robin DNS

. cache purge: purges 20 pages at once. is that a good number? try others
    o maybe the number should depend on the page size?

o delta updates (DU)
    - insert SMOs (splits) are applied when "going down"
        - move split operation to "the janitor"
    - erase SMOs are vastly simplified (only delete pages; no merges, no shifts)
        -> leaf nodes can become empty
        -> internal nodes must at least set ptr_down
        -> janitor: only merge pages if they have the same parent and if they
            are both nearly empty
            - merge goes from top to bottom
            - if there's an underflow: the path to this underflow is added
                to the janitor's queue
        -> in the end there's a btree skeleton with empty leafs and nearly-
            empty nodes; this can then be compact()ed offline or asynchronously
        - is there a cheap way to visualize the tree?
            -> yes: http://www.graphviz.org/content/datastruct
    - SMOs are separated from the actual operation
        -> check the literature
        http://pdf.aminer.org/000/409/763/b_trees_with_relaxed_balance.pdf
    - reorg on demand (new compact() API) or in background? better in
        background. then come up with very simple rules to merge empty pages
        and clean up the tree.
        -> just rinse and repeat: merge children with the same parent (see
            above). The merges will start at the bottom and then slowly move
            up. repeat till there's no more merging to be done. just figure
            out a way how to do that efficiently!

    - the actual operation now becomes atomic and simply attachs a DU to
        the modified page
        - creating a DU requires a single memory allocation!
    - find a single solution for transactions AND regular updates
        is this possible? they both serve different purposes, and the
        consolidation could become very tricky (think of cursors, 
        approx. matching...)
        -> but maybe the txn opnode/op could *point* to the DU?
        - the janitor makes sure that DUs of aborted txn's are silently removed
    - the janitor merges the DUs if
        - the number of DUs exceeds a certain limit
        - the page is merged or split
        - the page is flushed
        - there is a lookup operation
        - however: skip those of uncommitted/aborted transactions!
    - how does recovery work? all DUs that were already flushed (can find out
        by looking at their lsn) are ignored; only those are applied that
        are not yet written to disk
        - each journal entry of a DU must store the current lsn AND the txn-id
        - whenever a page is flushed then the page must contain the lsn
            of the latest DU that was merged!

o improve: ham_bench --cache=310241024 --reopen=true --key=binary --keysize=64 --pagesize=1024 --recsize=0 --bulk-erase --distribution=random --extkey-threshold=20 --recsize=0 --stop-ops=1000000 --seed=1391686926

o can we come up with olap functions that operate directly on the btree data?
    -> working on keys only!! does that make sense?
    - should be nested, i.e. TOP(20, SELECT(SELECT(t1, p1), t2))
    o MIN()
    o MAX()
    o TOP(N, predicate)
    o COUNT(predicate)
    o AGGREGATE(predicate, result)
    o AVERAGE(predicate, result)
    o SELECT(predicate, callback)
    o JOIN(predicate, other table, join-callback, result-callback)
    -> look for erlang map functions (foldl, foldr etc) for interfaces
    -> the SELECTs should return VIEWs which then can used as input for other
        SELECTs; this way you can build a "tree" of statements:
        SELECT(callback1,
            JOIN(callback2, table1,
                SELECT(callback3, table2)))

o improve integration of static code analysis in release process
    o oclint
    o coverity

high-level plan for 2.1.7 ...................................................

- page flushes/cache purges/flushing txns is handled asynchronously in the
    background
    TODO TODO TODO

- reduce the linked lists - they're hard to be updated with concurrent
    operations
    o page
    o transaction and dependent objects
    o ...


- prefetch cache lines when searching
- linear SIMD search -> PRO
- compress the log with snappy -> PRO
- compress the records with snappy -> PRO
- encrypt the log and the environment file with AES -> PRO
- hot backups (vacuumizes to a different file) -> PRO
- bulk updates -> PRO
    - give users API to allocate memory for keys/records
    - if user says that data is already sorted then do not re-sort
    - add those delta updates to the txn-trees or to the stream
- first PRO release
    o webpage updates, PR, mailings

- compression -> PRO
- cache-oblivious page distribution?
    http://supertech.csail.mit.edu/cacheObliviousBTree.html
- bloom filter -> PRO
- concurrency
- operations on compressed data (COUNT(), MIN(), MAX(), ...)?
- C-Store style projections? (requires complex schema types)
    - introduce a new library with complex schema types, projections
    - analytic functions
        count, min, max, sum, product, average, ln, sqrt, exp, round, trunc,
        date/time functions and interval functions
    - select(predicate_t, column_descriptor_t, select_state_t)
             \- an AST of functors
                          \- existing columns or generated columns (i.e. sum())
                                                \- keeps track of offset, count
    - erase(predicate_t)
    - explain(predicate_t, column_descriptor_t)

o introduce PRO version
    o start a closed repository
    o one source base with different licensing headers, different licensing
        string (also for tools), different version tag
    o API to get licensee information (is_pro_version())
    o new release process
    o prebuilt win32 files
    o get rid of serial.h - it's not really required and only creates efforts
    o how to share files with customers? need a login area,
        downloadable, customized files (win32, serial.h, tarballs...)
        -> send out mails if a new file is available
    o evaluation license: build binaries for the most common architectures
        o insert expiration date checks
        o special copyright strings
        o prebuilt for win32/win64
        o unix: obfuscated source code
        o need an automated flow for signups, for evaluation licenses etc
    o extra documentation
    o define file format interoperability?
    o what are the minimum features required for the first release?
        - (evaluation licenses)
        - prefix compression for strings
        - lightweight compression for binary keys
        - SIMD for searches
        - AES encryption
        - hot backups

o PRO: btree can compress keys
    x get rid of the whole minkey/maxkey handling because these numbers
        are irrelevant with compression
    o try to reduce the changes to a new KeyProxy object
    o prefix-compression for strings
        o each 2kb have a full string (indexed by skiplist)
    o delta-compression for numeric data types (>= 32bit)
        (can this be combined with a bitmap compression? the deltas are
        compressed in a bit-stream? but then we end up with variable
        length encodings...)
    o lightweight compression for keys
        http://oldhome.schmorp.de/marc/liblzf.html
        http://lzop.org
    o record compression for blobs (lzop.org? snappy?)
        better postpone this and compress all pages in the lss
    o do we need delta updates for efficient inserts? - i think not yet...

o PRO: use SIMD for fixed-length scans and __builtin_prefetch
    o use valgrind to track cache misses
    http://pcl.intel-research.net/publications/palm.pdf
    http://www.cs.toronto.edu/~ryanjohn/teaching/csc2531-f11/slides/Ioan-SIMD-DBMS.pdf
    http://gcc.gnu.org/onlinedocs/gcc-3.3.6/gcc/Other-Builtins.html
    http://stackoverflow.com/questions/8460563/builtin-prefetch-how-much-does-it-read
    http://tomoyo.sourceforge.jp/cgi-bin/lxr/source/include/linux/prefetch.h
    http://stackoverflow.com/questions/7327994/prefetching-examples
    o use linear search with fixed length keys (if max-keys-per-page
        or the subrange in the binary search is low enough, and if the
        performance makes sense) -> also for MIT
    o if both layouts use binary search then move it back to the proxy!

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

o btree_impl_default::set_record: if the duplicate is LAST of the last key
    in the node then simply append the record and increase next_offset

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o flush transactions in background (when the btree is concurrent)

o Improve leaf pages caching
    Store start/end key of each leaf page in a separate lookup table in order
    to avoid btree traversals. This could be part of the hinter.
  - one such cache per database
  - should work for insert/find/erase

o allow transactions w/o journal

o allow transactions w/o recovery

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o when flushing the Changeset: batch ALL changes for the WHOLE transaction,
    then flush all of them together. This way we can "merge" multiple changes
    for the same page.
    Also review the whole flush process - when not to log etc.
    - only 1 page affected: no need to log it because it is idempotent
    - freelist pages are always idempotent
    - more than 1 index page? not idempotent (most likely)
    - more than 1 blob page? not idempotent (maybe)
    o define a few benchmarks
    o be careful: if N operations are modifying the same changelog, and
        then #N+1 aborts then the aborting operation must NOT clear the
        changelog!

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o changeset: instead of simply adding pages to the changeset, the caller
    could already specify whether this page needs logging or not;
    i.e. after freelist rewrite, the blob pages do not need logging if a
    blob is deleted  

o is there a way to group all changeset flushes of a Transaction into one
    changeset, and batch-commit multiple commits? that way we would avoid the
    frequent syncs and performance would be improved
    o would have to remove all of assert(changeset.is_empty())
    o but we can use that assert prior to txn_begin

o flush in background (asynchronously)
    o need new flag file HAM_DISABLE_ASYNCHRONOUS_FLUSH
    o if in-memory database: disable async flush
    o if transactions are disabled: disable async flush
    o if enabled: create background thread, wait for signal
    o ham_env_flush: if txn are enabled then try to flush them to disk
    o how to deal with an error in the background thread???
        o store in Environment, then return in every exported function
    o default: async flush is OFF!

    o extend monster tests
        o with async flush
        o without async flush
        o extend/run performance test
        o run monster tests

    o documentation
        o tutorial
        o faq

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    -> or: make this the default; call the new flag HAM_TXN_MAYBE_WILL_ABORT
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o ham_get_count: could be atomically updated with every journal entry

=======
>>>>>>> Updated TODO
