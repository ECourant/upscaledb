 
I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------

As a user i want to run many Transactions in parallel with high performance.
I'm not interested in multiple threads (yet), although i may use several 
threads (but then i'll use synchronization in my own code).
==============================================================================

x extkey: use a generic hash table

x the changeset flushes its pages with WRITE_THROUGH - but that causes
    fsync?! - no, it's not. i renamed the flag.

x rewrite cursor_move (once more) 
    x currently there are too many compares of the cursors - decrease them
    x cursor_move_last is working
    x cursor_move_first is working
    x cursor_move_next
    x cursor_move_previous
    x two tests are failing (see README)
    x current problem: dupecache is always created even if duplicates are
        not used - no problem, it's a NOP if dupes are disabled
    x _local_cursor_create can directly return the new Cursor
    x _local_cursor_clone can directly return the new Cursor
    x the cursor is still a mishmash of C functions with a C++ object
        -> move all functions (i.e. cursor_set_to_nil, cursor_move...) into
        the namespace of the class
        x __compare_cursors -> int Cursor::compare(flags);
    x new test case:
        insert (1)
        insert (2)
        cursor_move_last
        cursor_move_previous
        cursor_move_previous -> KEY_NOT_FOUND
        insert (0)
        cursor_move_previous -> OK
        -> and the same with first/next
        x clarify documentation

x run performance tests w/ transactions, duplicates and cursors
    valgrind --tool=callgrind ./test --use-transactions=200 --duplicate --use-cursors  --progress ../../testfiles/1/180.tst
    x flatten txn_op_t
    x txn_tree_get_or_create (maybe ALWAYS create the tree?)
    x cursor_erase: no need to purge the cache
    x local_cursor_insert calls __btree_cursor_points_to, which is very
        costly
    x local_cursor_find must not call update_dupecache - not possible :(
    x local_cursor_find must not call cursor_sync - not possible :(
    x local_cursor_insert must not require a cursor->update_dupecache - not
        possible :(

x run performance tests w/ transactions and w/o duplicates
    01-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-01-00 ./test --use-transactions=200 ../../testfiles/1/180.tst

x run performance tests w/ transactions and w/ overwrite
    02-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-02-00 ./test --use-transactions=200 --overwrite ../../testfiles/1/180.tst

x run performance tests w/ transactions, duplicates and w/o cursors
    03-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-03-00 ./test --use-transactions=200 --duplicate ../../testfiles/1/180.tst

x run performance tests w/o transactions and w/o duplicates
    06-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-06-00 ./test ../../testfiles/1/180.tst

x run performance tests w/o transactions and w/ duplicates
    07-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-07-00 ./test --duplicate ../../testfiles/1/180.tst

x run performance tests w/o transactions and w/ duplicates and w/ cursors
    08-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-08-00 ./test --duplicate --use-cursors ../../testfiles/1/180.tst

o run performance tests w/ transactions and cursors and duplicates
    05-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-05-00 ./test --use-transactions=200 --use-cursors --duplicate ../../testfiles/1/180.tst
    o btree_cursor_points_to_key is the big blocker

o run performance tests w/ transactions and cursors
    04-00
valgrind --tool=callgrind --callgrind-out-file=callgrind-2.0.0-04-00 ./test --use-transactions=200 --use-cursors ../../testfiles/1/180.tst
    o uses Cursor::get_dupecache_count and Cursor::clear_dupecache,
        although duplicates are disabled

o improve performance
    o use writev in journal
    o use tcmalloc
        o OR use lookaside list in memory allocator
    o changeset.clear should be O(1), use std::vector
    o prepare_key/prepare_record: inline or macro?
    o Why ever call changeset::get_page when fetching a page??
    o 08: local_cursor_erase calls Cursor::set_to_nil, but btree_cursor_erase
        also sets to nil
    o 08: " also calls btree_cursor_uncouple, but why? for the current cursor?
        that's not necessary because it's anyway set to nil

o duplicate (extended) keys are horribly slow, check why and improve it
    x check if 2.0.0rc1 had the same problems (yes, it has)
    --> __read_chunk is the bottleneck
        -> called from blob_read (through db_get_extended_key, 
                            btree_read_record)
        -> called from __get_duplicate_table
    ./test --use-transactions=200 --duplicate --use-cursors 
            --verbose=1 ../../testfiles/1/202.tst

o improve journalling and get rid of the physical log as much as possible
    . recovery: recreate all pending transactions (if requested)
    . api function to get a list of pending transactions after recovery
    o be careful - even if the btree operations are atomic, the whole
        insert/erase is not because it also affects the blob area and the
        freelist (and maybe the header page, if the root page address is
        modified)
        -> this is where we might have to rewrite the btree insert algorithm
    o do this for ham_insert without SMOs
    o when inserting a blob (ham_insert) we NEVER have to log the blob pages.
        if blob_alloc fails then just return the error
    o do this for ham_erase without SMOs
    o what about blobs? if blobs are immediately written to disk then 
        they do not be logged, and we do not need undo information
        alternatively: the changeset first flushes all blobs without
        logging them. all other pages are logged, then flushed (if there's
        more than one page left). 
        - actually we could just writev the blobs and bypass the cache
        - what about the freelist??

o use writev when writing blobs directly to disk

o document how to optimize for performance
    o with transactions: try not to use duplicates
    o use overwrite as often as possible

o new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

o compare performance against 1.1.14 (with and without Transactions)
    o performance: check for DB_NEW_PAGE_DOES_THRASH_CACHE in env.c. If the page
        is only required once then it should be inserted at the tail of the
        totallist, like a page that was not accessed for a very long time
    o check performance of extkeys (no txn's, no dupes) with 1.1.14
        -> increase the age of the cache

o API changes for 2.0.0
    o ham_create_db, _erase_db (and others) get a txn parameter (reserved)
    o can we remove some of the deprecated functions?
    o ham_txn_begin: receives a name (reserved), a parent pointer (reserved)
        and the Env as parameter (not the db!)
    o what else?
    o move all deprecated functions and macros into a separate file; only 
        include it if -DHAM_USE_DEPRECATED
    o increase libtool version
    o ham_txn_begin receives Environment handle, not Database handle
        o need a new parameter (reserved) for parent txn
        o need a new parameter for txn name (string)
            o this needs a getter function
        o update unittests/add new tests
        o update C++ API
        o rewrite auto-abort/auto-commit -> move to env
        o what happens if a database is closed, but it's modified by a txn
            that is still active? -> error (verify this!)
        o need to increase libtool version!

o test with tcmalloc; if it works then also use it in the master branch, but
    make sure that memory consumption does not increase significantly




o c++ API
    o make sure everything is updated
    o clean up usage of ham_parameter_t - overload functions for bool, u64 
        and strings
        (i want the high-level APIs to use the c++ API and not the C one)

o java API
    o update and rewrite to use the C++ API

o python API
    o update and rewrite to use the C++ API

o iOS port - try to include in main branch

o Android port - try to include in main branch

o create a facebook page for hamsterdb

o guru.com/twitter: look for someone who can write a PHP or Perl or Ruby
    wrapper

o implement support for partial keys?

o update the tutorial

. there are a couple of areas where a btree cursor is uncoupled, just to 
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. hash-table.h: the foreach/remove_if/visitor pattern is clumsy. use 
    functor or class w/ operator() instead
. changeset: use a generic hash table for fast lookup (but leave list in place
    for everything else)
. cache: use a generic hash table

o device, page and os shold no longer return errors but throw exceptions

o journal: use writev if it turns out to make sense

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

o the whole c++ protocol should be c++-ified

o ./test --use-transactions=10 ../../testfiles/4/blb-002.tst
    fails with an assert if disk is full; need to make sure that recovery 
    works fine in such cases, i.e. with an ErrorInducer

    os_posix.cc[221]: pwrite() failed with status 28 (No space left on device)
    env.cc[1606]: failed to flush op: -18
    ASSERT FAILED in file env.cc, line 1519:
        "txn_op_get_flags(op)!=TXN_OP_FLUSHED"
    db[0]: txn_begin failed w/ status -18

. move the whole configuration (key sizes, parameters, page size, etc) into a 
    separate class which is instantiated by the env

. c++-ify the page

. c++-ify the device

. c++-ify the btree node representation; 
    o include duplicates as well! disentangle duplicates from blob-handling
    o c++-ify blob-handling (separately from duplicates)

XXXXXXXXXXXXXXXXXXXXX release 2.0.0 rc3 UNSTABLE XXXXXXXXXXXXXXXXXXXXXXXXXXX

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

o recovery: re-create pending transactions (if required)
    o needs a function to enumerate them

. new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the blob, not
    to the log. the log will only contain the rid.
    o document this (and also the drawback - that an abort will lose the 
        blobs and they cannot be reused
    -> this affects all temporary ham_insert-transactions 
    (not sure if this should get high priority)

o test with lessfs (and add lessfs to the monster testsuite)
    o test with reboots - like those that made problems with lessfs

o hamsterdb.com
    x add twitter feed
    o API documentation: don't link to "modules" but to startup page, update
        with newest version
    o crupp.de: do a backup of the database
    . google +1 button
    . can we use something like Aller.font?

o update documentation
    x in header file
    o in the wiki
        o don't forget to list all functions that are currently disabled
            w/ txn's -> sorting dupes, approx. matching, ...
        o transactional behavior/conflicts of duplicate keys
    o in the wiki: start with internal documentation
        o transactions
        o architecture
        o btree
        o journal/log
        o cache
        o I/O
        o unittests
        o cursor(s)
        o monstertests - how to use them?

o fully (!) automize the whole release process for unix; the result (on
    success) are the following files:
    o tar-ball
    o the README
    o the documentation as a tar
    o the Changelog
    o the release notes (a template)
    o the output (xml) of the monster tests

. port to WinCE

o how can we extend the monster-tests to have reliable tests for transactions?

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created

o extkeys: don't use txn_id for the 'age', use lsn instead

. allow use of transactions without a log/journal

. allow use of transactions for in-memory databases

XXXXXXXXXXXXXXXXXXXXX release 2.0.0 rc4 UNSTABLE XXXXXXXXXXXXXXXXXXXXXXXXXXX

o ham_get_count: could be atomically updated with every journal entry

o when flushing a changeset: sort by offset, use writev()

o add concurrency (on a high level)

o flush transactions in background

. have a flag to disable flushing of logfiles/journal files (or flush them 
    async.)

o continue as described in integrate-ham2.txt...

