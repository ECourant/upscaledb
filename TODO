 
I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------

As a user i want to run many Transactions in parallel with high performance.
I'm not interested in multiple threads (yet), although i may use several 
threads (but then i'll use synchronization in my own code).
==============================================================================

o rewrite cursor_move (once more) 
    idea for algorithm:
        last: 
            btree_cursor_move(last)
            txn_cursor_move(last)
            if (txn_cursor>=btree_cursor)
                couple_to_txn
            else
                couple_to_btree
            cursor.last_cmp=cmp

        previous_single: -- always move a SINGLE step (although conflicts
                         -- are skipped). all other errors and states are 
                         -- handled by the caller - erased keys, overwritten 
                         -- keys, duplicates. the compare value is cached.
            if (cursor.last_cmp<0)
                txn_cursor_move(previous);
            else 
                btree_cursor_move(previous);
            cursor.last_cmp=compare_cursors()

            if (cursor.last_cmp<0) {
                if (is_conflict())
                    return (previous(cursor))
                // all other error codes (i.e. erased, eof...) are
                // handled by caller)
                couple_to_btree
            }
            else {
                if (is_conflict())
                    return (previous_single(cursor))
                // all other error codes (i.e. erased) are
                // handled by caller)
                couple_to_txn
            }

        previous(cursor):
            if (cursor.dupe_index>0) -- pointing to duplicate?
                -- read updated dupecache from global dupecache - it might
                -- have been invalidated in the meantime
                dupecache_entry=get_and_update_duplicate(cursor) 
                return previous_duplicate(dupecache_entry)

            while 1:
                previous_single(cursor)
                if db.duplicates_enabled:
                    // uses global dupecache
                    dupecache_entry=update_duplicates(cursor) 
                    if dupecache_entry.empty()
                        continue
                    else
                        __cursor_move_to_last_dupe()
                        break
                if (coupled_to_btree)
                    if btree.is_erased:
                        continue
                    if btree.is_overwritten:
                        couple_to_txn
                        break
                    else
                        break
                elif (coupled_to_txn)
                    if txn.is_erase:
                        continue
                    break
            return (key/record)

    o two tests are failing (see README)
    o too many compares of the cursors
    o duplicate keys are slow (need a new dupecache which is shared between
        cursors)
        -> needs to be invalidated when inserting new keys, erasing them
    o new test case:
        insert (1)
        insert (2)
        cursor_move_last
        cursor_move_previous
        cursor_move_previous -> KEY_NOT_FOUND
        insert (0)
        cursor_move_previous -> OK
        -> and the same with first/next
    o new test case for cursors
        insert (1, a)
        insert (1, b) (duplicate of 1)
        move (last) (-> 1, b)
        insert (1, c)
        move (last) (-> 1, c)? is the dupecache updated correctly?
    o there are a couple of areas where a btree cursor is uncoupled, just to 
        retrieve the key and to couple the txn-key. that's not efficient
            db.c:__btree_cursor_points_to
            db.c:__compare_cursors
            txn_cursor.c:cursor_sync
            txn_cursor.c:cursor_overwrite
        o move to a separate function
        o try to optimize
    o duplicate (extended) keys are horribly slow, check why and improve it
        x check if 2.0.0rc1 had the same problems (yes, it has)
        --> __read_chunk is the bottleneck
            -> called from blob_read (through db_get_extended_key, 
                                btree_read_record)
            -> called from __get_duplicate_table
        ./test --use-transactions=200 --duplicate --use-cursors 
                --verbose=1 ../../testfiles/1/202.tst
        o duplicate tables exist in the btree-cursor AND in the high-level
            cursor. can we eliminate the btree cursor cache, or at least make
            sure that tere's no performance penalty when using both?
            can we cache the duplicate tables??
    o the cursor is still a mishmash of C functions with a C++ object
        -> move all functions (i.e. cursor_set_to_nil, cursor_move...) into
        the namespace of the class

o compare performance against 1.1.13 (with and without Transactions)
    o do some profiling - i am afraid that changeset_get_page() is not efficient
        enough; maybe a hash table is better?
    o performance: check for DB_NEW_PAGE_DOES_THRASH_CACHE in env.c. If the page
        is only required once then it should be inserted at the tail of the
        totallist, like a page that was not accessed for a very long time

o API changes for 2.0.0
    o ham_create_db, _erase_db (and others) get a txn parameter (reserved)
    o can we remove some of the deprecated functions?
    o ham_txn_begin: receives a name (reserved), a parent pointer (reserved)
        and the Env as parameter (not the db!)
    o what else?
    o increase libtool version

o improve journalling and get rid of the physical log as much as possible
    o recovery: recreate all pending transactions (if requested)
    o unittests
    o merge log and journal. There's no need to have two different files. 
        For all 'simple' operations (insert, erase w/o SMO) a journal entry is 
        sufficient. For all other operations we can just append the log
        entries to the journal file. in such cases, the journal entry of an
        insert or erase (w/ SMO) contains the modified pages as well.
        ??? really? does that simplify things?
    o api function to get a list of pending transactions after recovery
    o be careful - even if the btree operations are atomic, the whole
        insert/erase is not because it also affects the blob area and the
        freelist (and maybe the header page, if the root page address is
        modified)
        -> this is where we might have to rewrite the btree insert algorithm

. test with tcmalloc; if it works then also use it in the master branch, but
    make sure that memory consumption does not increase significantly

o journal: use writev if it turns out to make sense

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

o the whole c++ protocol should be c++-ified

o ./test --use-transactions=10 ../../testfiles/4/blb-002.tst
    fails with an assert if disk is full; need to make sure that recovery 
    works fine in such cases, i.e. with an ErrorInducer

    os_posix.cc[221]: pwrite() failed with status 28 (No space left on device)
    env.cc[1606]: failed to flush op: -18
    ASSERT FAILED in file env.cc, line 1519:
        "txn_op_get_flags(op)!=TXN_OP_FLUSHED"
    db[0]: txn_begin failed w/ status -18

. move the whole configuration (key sizes, parameters, page size, etc) into a 
    separate class which is instantiated by the env

. c++-ify the page

. c++-ify the device

. c++-ify the btree node representation; 
    o include duplicates as well! disentangle duplicates from blob-handling
    o c++-ify blob-handling (separately from duplicates)

XXXXXXXXXXXXXXXXXXXXX release 2.0.0 rc3 UNSTABLE XXXXXXXXXXXXXXXXXXXXXXXXXXX

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

o new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the blob, not
    to the log. the log will only contain the rid.
    o document this (and also the drawback - that an abort will lose the 
        blobs and they cannot be reused
    -> this affects all temporary ham_insert-transactions 
    (not sure if this should get high priority)

o test with lessfs (and add lessfs to the monster testsuite)
    o test with reboots - like those that made problems with lessfs

o ham_txn_begin receives Environment handle, not Database handle
    o need a new parameter (reserved) for parent txn
    o need a new parameter for txn name (string)
        o this needs a getter function
    o update unittests/add new tests
    o update C++ API
    o rewrite auto-abort/auto-commit -> move to env
    o what happens if a database is closed, but it's modified by a txn
        that is still active? -> error (verify this!)
    o need to increase libtool version!

o hamsterdb.com
    x add twitter feed
    o API documentation: don't link to "modules" but to startup page, update
        with newest version
    o crupp.de: do a backup of the database
    . google +1 button
    . can we use something like Aller.font?

o update documentation
    x in header file
    o in the wiki
        o don't forget to list all functions that are currently disabled
            w/ txn's -> sorting dupes, approx. matching, ...
        o transactional behavior/conflicts of duplicate keys
    o in the wiki: start with internal documentation
        o transactions
        o architecture
        o btree
        o journal/log
        o cache
        o I/O
        o unittests
        o cursor(s)
        o monstertests - how to use them?

o fully (!) automize the whole release process for unix; the result (on
    success) are the following files:
    o tar-ball
    o the README
    o the documentation as a tar
    o the Changelog
    o the release notes (a template)
    o the output (xml) of the monster tests

. port to WinCE

o how can we extend the monster-tests to have reliable tests for transactions?

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created

o extkeys: don't use txn_id for the 'age', use lsn instead

. allow use of transactions without a log/journal

. allow use of transactions for in-memory databases

XXXXXXXXXXXXXXXXXXXXX release 2.0.0 rc4 UNSTABLE XXXXXXXXXXXXXXXXXXXXXXXXXXX

o when flushing a changeset: sort by offset, use writev()

o flush transactions in background

. have a flag to disable flushing of logfiles/journal files (or flush them 
    async.)

o continue as described in integrate-ham2.txt...

