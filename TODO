
x install new uqi header file, remove old stuff

x manage plugins (globally, not per Environment!)
x load plugins via uqi_register_plugin
x load plugins from external libraries
x add unittests
    x negative test: access a plugin which does not exist
    x negative test: import from a library which does not exist
    x negative test: import from a library which does not export the symbol
    x negative test: import from a library which exports, but not the
        requested plugin
    x negative test: set a bad plugin version (load must fail)
    x create a library with a plugin
    x load the plugin from the library
    x type must be UQI_PLUGIN_PREDICATE or UQI_PLUGIN_AGGREGATE
    x verify functions based on type
    x verify that the plugin was registered correctly
x write the parser; move it to a separate c++ file. it should fill a
    SelectStatement object
    x unittests
        x negative test: check syntax errors
        x negative test: use unknown external plugins
        x negative test: use unknown function
        x test various query strings and verify the SelectStatement

x implement the uqi_select function: calls select_range

o implement the uqi_select_range function
    o open the database, then close it again
        x implement
        o unittest
    o if already open: use existing handle
        x implement
        o unittest
    o fail if the database does not exist
        x implement
        o unittest
    o make sure that the cursor(s) are from the selected database!
        x implement
        o unittest

    x move implementation details to the LocalDatabase class
    x split implementation into different sections:
        1) txn enabled? process (optional) transactional data at the beginning
            (while cursor.coupled_to_txn()) { process key; move next }
        2) txn enabled? process mixed nodes
            if (node.contains(txnkey) ? use cursor : process btree
        3) process pure btree nodes
            node.scan(...)
        4) txn enabled? process (optional) transactional data at the end
            (while cursor.coupled_to_txn()) { process key; move next }

x implement SUM, DISTINCT SUM, SUM WHERE, DISTINCT SUM WHERE
x implement AVERAGE, DISTINCT AVERAGE, AVERAGE WHERE, DISTINCT AVERAGE WHERE
x implement COUNT, DISTINCT COUNT, COUNT WHERE, DISTINCT COUNT WHERE
x SUM and AVERAGE must use different accumulator members (uint64_t OR double!)
    depending on the key type!
x Need a ScanVisitor which acts as a plugin proxy
x LocalDatabase::select_range must use the new ScanVisitors!
x move uqi-related code to 4uqi

x restore db6.cpp

o re-activate tests in uqi.cpp and zint32.cpp

o implement COUNT for binary keys with fixed- and variable length
    -> force use of cursors for var-length keys?
o add new tests which use plugins
o also test with binary plugins (fixed length and variable length)

o function/predicate names should ignore the case -> always convert to
    lower case when parsing and when importing a plugin

o the 'end' cursor is currently ignored. Implement it!
o the 'limit' setting is currently ignored. Implement it!

o remove Database::scan
o what happens with ups_db_count?

o add to remote API
o add to java API (w/ sample)
o add to dotnet API (w/ sample)
o add to erlang API (w/ sample)
o add to python API (w/ sample)

o parser: allow hex-and octal-style integers ("... from database 0x1")
    o add unittests

o add numeric record types
o allow queries on records (but not yet mixed queries w/ keys/records)

o allow numerical queries (SUM, AVERAGE...) only on numerical data!
o ... but others are always allowed

o add samples
    o also add to webpage
o add documentation
    o extend the tutorial

o release 2.2.0
  o advertise the new features on the front page!

o add mixed queries (i.e. SUM($record) where PREDICATE($key))
o add integer record compression (simdfor)

------------------- idea soup ---------------------------------------------

o remove dependency to libuv 1.0, use boost::async instead (makes the build
    process a lot smoother)

o clean up BtreeIndex class; it has too many responsibilities, i.e.
    managing configuration, persisting configuration, btree traversal,
    btree actions etc
    o the configuration of all btrees is moved to a separate class
    o the BtreeIndexFactory can create or open btree indices; it manages
        the page with the persistent configurations
    o the BtreeIndex has a persistent configuration, a runtime configuration
        (DatabaseConfiguration) and entry points for btree actions
    o create a common root class for BtreeActions with common functions,
        i.e. traversal

o Refactoring: rewrite the whole cursor layer
    o clean up the public interface
    o remove the Transaction cursor, merge BtreeCursor with LocalCursor
    o there should be 3 states:
        - nil
        - coupled to btree
        - coupled to txn
        o state() - returns the state
        o set_state() - changes the state
    o key() returns the current (coupled) key
    o record() returns the current (coupled) record
    o get rid of the DuplicateCache and the duplicate index in the cursor; if
        the DeltaUpdates are correctly sorted, then the cursor should not be
        necessary (unless the duplicate position is explicitly required via
        ham_cursor_get_duplicate_position())

o More things to refactor in the btree
    o EraseAction uses duplicate_index + 1, InsertAction uses duplicate_index
        -> use a common behaviour/indexing
    o EraseAction line 71: if the node is empty then it should be merged and
        moved to the freelist!

o when splitting and HAM_HINT_APPEND is set, the new page is appended.
    do the same for prepend!

o Refactoring: all unittest fixtures should derive from a BaseFixture,
    which creates an Environment, creates a list of databases (w/ parameters),
    and if required also a cursor, a transaction and a context
    o include additional management functions like lenv(), ldb(), ltxn(),
        page_manager(), cache(), context()...
    o what else?
    o then reorganize the tests
        - public API
        - internal modules

o The PageManager state is currently stored in a compressed encoding, but
    it is less efficient than the standard varbyte encoding because
    pages > 15 * page_size have to be split. Use a standard vbyte encoding
    instead (it will anyway be required later on).

o Implement record compression - a few notes
    ByteSlice: Pushing the Envelop of Main Memory Data
    Processing with a New Storage Layout
    http://delivery.acm.org/10.1145/2750000/2747642/p31-feng.pdf

    1) the user defines the record structure.
    2) an optimization stage reorders the record columns to optimize storage
        (i.e. with dynamic programming)
    3) SIMD code is generated on the fly to pack, unpack records and single
        elements (see http://stackoverflow.com/questions/4911993/how-to-generate-and-run-native-code-dynamically or ask Ben/Andi...)
    4) Pack like PAX (group all column values together) or each record
        standalone?

o look for a better compression for DefaultRecordList, i.e.
    - Each group is a GroupedVarInt w/ 4 bits per entry; a 64bit
        number can then hold flags for 16 numbers
        -> (but maybe increase this to hold at least 32 or 64 numbers, to
            reduce the overhead ratio)
    o create a GroupedVarInt<Max, T> class, where |Max| is the maximum number
        of elements that are grouped, and T is the type of these elements
        (i.e. uint64_t)
        -> memory is managed by the caller
        -> the state (i.e. used block size etc) is stored externally, and
            managed by the caller
        o append a key
        o prepend a key
        o insert a key in the middle
        o grow blocks
        o split blocks
        o can perform copies w/o re-compressing

    o try to move the Zint32 index to a base class
    o Use small index which stores offset + bits for each group
    o a separate bit is used to signal whether the (compressed) number is
        a record id
    o avoid ripple effects by growing/splitting the block

o use compression also for duplicate records
    i.e. use GroupedVarint for inline duplicates

o Concurrency: merge BtreeUpdates in the background

o should we have a separate "recsize == 0" RecordList for duplicates?
    they could only store the duplicate count (but should be able to deal
    with duplicates that are > 256!)
    -> requires grouped varints

o asynchronous prefetching of pages
    -> see posix_fadvise, libprefetch

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurÃ¼ckgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

