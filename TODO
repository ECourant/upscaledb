I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

==============================================================================

x the PageManager class is becoming a beast! Split it into...
    x Freelist (implementation class)
    x PageManagerWorker
    x Should the PageManager own the Device? - no, doesn't make sense
    x the PageManagerWorker only accesses the Device and the Cache as external
        objects
        x The PageManager needs to be synchronized
        x Cache (not synchronized!)
        x Device (synchronize it!)
        x Once more review the PageManager and the Worker

x clean up threading issues
    - ./ham_bench --cache=50 fails frequently
    - creating deep copies of locked pages leads to test failures
    - valgrind reports too many issues
    
    The following locked entities are defined:
    - Page: has a spinlock which signals whether the page is in use or not
    - PageManager: serializes access to the caches and locks the pages
    - The lock order is: Environment -> PageManager -> Page

    x From caller's perspective, a Message can be enqueued blocking or
        async
        x no different messages required
        x no notify() call required in the queue handler
    x PageManagerWorker: implement all functions in the style of kPurgeCache
        (rename to kFlushPages, pass a list of page IDs, reuse for Changeset,
        closing databases and everything else)
        x CloseDatabase
        x PurgeCache
        x FlushPages (when is this used? - rename to
                PageManager::flush_all_pages())
        x merge all three into a single message
        x PurgeCacheMessage: keep as class variable; use FlushPagesMessage
            type and "result" variable for storage
        x synchronous messages should return a result
        x FlushChangeset (can this be async? what happens if it fails?)
    x If the worker throws then the message object is leaked
    x Review the PageManager interface; must be serialized
    x Review the PageManager implementation; must follow the lock order
        as described above
    x Clean up CacheLine - it's just a PageCollection
    x make sure that ALL flushes happen in the worker thread!
    x then switch to SpinLocks instead of mutexes
    x ChangesetFlush: has many helgrind warnings 
    x Journal/recoverFromRecoveryTest fails (release build and w/ mutex)
    x run monster tests
        x failures when opening existing db: integrity check failed in page 0x20c6400: record of item #8 is in freelist
        x could be a bug in the freelist
        x there should not be ANY concurrency right now because every async
            code is disabled and synchronized (w/ exception of changeset flush)
=======================================================
    o currently, all async operations are synchronous in the main thread
        x keep them synchronous, but move them to the worker thread
            x rewrite worker thread using boost async
            x verify that cache purge works
            x use pointers to transfer parameters to thread
        o then - piece by piece - make them async again
            (one problem was that close_db was async and therefore executed
            BEFORE a changeset flush was finished
            o cache flush
            o changeset flush
    o switch to SpinLocks instead of mutexes
        o once more run monster tests
=======================================================

o zint32: use FOR encoding (w/o SIMD)
o zint32: use FOR encoding (w/  SIMD)
o zint32: create custom insert function for StreamVbyte
o zint32: create a script which runs all benchmarks
o zint32: create good looking graphs with the results
o zint32: continue with the paper

o investigate performance issue; when reopening the database operations
    are much slower. why?
    ./ham_bench --use-berkeleydb=true --reopen=true  --key=binary --keysize=64 \
        --pagesize=1024 --recsize=0 --bulk-erase --cache=104857600 \
        --stop-ops=50000
    ./ham_bench --use-berkeleydb=true --reopen=true  --key=binary --keysize=64 \
        --pagesize=1024 --recsize=0 --open --cache=104857600 --stop-ops=50000

o Win32: ham_recover is missing in the .zip-file

o There are performance issues related to cache flushes.
    1. if multiple pages are flushed then their locks are held for a long time
    2. even if a single page is flushed then it's possible that there's a
        significant latency spike when fetching a locked page
    A strategy to avoiding these spikes would be to create a copy of the page
    as soon as a page is fetched but the lock is already acquired.
    The original page could be hazard-collected after a while (10 sec), or
    forwarded to the worker thread for cleanup (as long as there's just one
    thread this will work fine).
    What if this (previous) page is in other linked lists?
    -> We need a way to swap the data of a page even while it is locked and/or
        flushed
    x combine mutex, is_dirty and the data pointer to a separate structure
    x try to avoid an extra memory allocation
    x the mutex only protects this structure!
    x the worker thread does not work on pages but on these structures
    x don't forget to free the structure, but only if it was allocated
    x ... and therefore the main thread can swap them if the page is locked!
    x compare performance - looks good, peaks are gone
    x fix unittests
    ===> implemented, but currenlty disabled. Causes test failures when
        running i.e. 1/45.tst with --cache=50 vs berkeleydb
    o run unittests with posix mutexes
    o run with helgrind
    o run monster tests

    o set breakpoint if spinlock spins more than 100 times; identify the
        bottleneck, try to improve

o ftruncate() allocates larger buffers than required, for performance
    reasons (ftruncate() causes I/O spikes). But then recovery.pl fails,
    therefore excess allocations were disabled if recovery is enabled
    (and on Win32)
    o enable again if recovery is enabled
    o disable on Win32 if mmap is used

o Investigate failure in erlang qc-tests

o PRO: files are hard to "diff" because each file is different from the APL
    version b/c of the license header. Change all license headers to a generic
    one, pointing to the COPYING file in the root directory - 
        - for APL
        - for commercial, proprietary, closed source
        - for the evaluation version
    o then rebase PRO

o Concurrency: flush the changeset asynchronously. As soon as the log
    is written (and fsync'd), the pages can be flushed to disk in the
    background. All pages are locked and forwarded to the worker thread.
    As soon as they are unlocked, they can be re-used again.
    x only pass the page_data to the worker, not the Page pointer!
        (because the page_data pointer can be replaced by the caller)
    x the worker thread then unlocks the pages afterwards
        -> not possible because thread B unlocks mutex which was locked
            by thread A
        -> then make it possible! i.e. by moving the mutex ownership to the
            worker thread
    o we HAVE to make sure that NO OTHER entity is allowed to flush pages,
        only the worker thread! otherwise we might have race conditions
        between the purger and the changeset-flush
        (remove member function hamsterdb::Page::purge and see where it fails,
        completely replace usage with the static version)
    x compare performance (before/after) - wow! up to 60% faster!
    x fix unittests
    x fix recovery tests
    x run monster tests

o Refactoring: completely decouple the Page from the Device; pass the Device as
    a parameter to fetch(), flush()
    - or vice versa, call device->flush(page), device->fetch(page)

o PRO: increase SIMD lookup performance (-> Meem)
    o look into SIMD Galloping Search (Lemire et al)
    o B. Schlegel, R. Gemulla, and W. Lehner. k-ary search on modern
        processors. In DaMoN, pages 52–60, 2009.
    o J. Zhou and K. A. Ross. Implementing database operations using simd
        instructions. In SIGMOD Conference, pages 145–156, 2002.
    o T. Willhalm, N. Popovici, Y. Boshmaf, H. Plattner, et al. Simd-scan:
        Ultra fast in-memory scan using vector processing units. PVLDB,
        2(1):385–394, 2009.

o refactoring of the whole transaction-mess
    goals:
      1. transactions can be created on the stack
      2. transactions are no longer a linked list
      3. transaction operations are not handled in a separate tree but
            are attached to the btree nodes
      4. remove that awful complexity that currently exists

    implementation steps:
      x create new structure DeltaUpdate - identical to TransactionOperation,
            but part of the Btree sources
      x SortedDeltaUpdates is a vector which is sorted by key and attached to
            a node (the node manages this vector)
      x UnsortedDeltaUpdates is a vector which is appended to only, no lookup
      x Each Transaction has a UnsortedDeltaUpdates
      x Each DeltaUpdate has a state (committed or aborted, txn-id)
      x As soon as a Transaction is committed or aborted then the state of
            its DeltaUpdates is updated
      x insert: check/update DeltaUpdates instead of TxnTree
        x perform a lookup on that key
        x check for conflict or if the key was erased before
        x if not: insert a new DeltaUpdate
      x erase: check/update DeltaUpdates instead of TxnTree
      x find: check DeltaUpdates instead of TxnTree
      x simple tests should work (no cursors, no splits/merges)
      x If a node is split or merged then the DeltaUpdates are re-distributed
            between the nodes
      x cursor: consolidate DeltaUpdates instead of TxnTree
        x insert
        x find
        x erase
        x move first
        x move last
        x move next
        x move previous
      o handle duplicate keys
        o consolidate DUs/btree duplicates
        o cursor: get duplicate count
        o cursor: move_first
        o cursor: move_last
        o cursor: move_next
        o cursor: move_previous
      o implement approx. matching
      o periodically merge committed deltas, free aborted deltas
        o make sure that attached cursors are coupled to the
            new btree item
        o if keys are deleted then 'nil' the cursors
            o this needs to be documented b/c it's a semantic change
      o temp transactions are created on the stack
        -> temp transactions are not stored in a linked list
        o (non-temp) transactions form a single linked list, not a double one
      o finally clean up the remaining code (rb.h, txn_index etc)

      o cursor: when deleting a key and the deleted key/record is fetched
            -> return HAM_KEY_NOT_FOUND
      o txn_cursor.cpp: either rewrite or delete
      o github issue tracker has more bugs regarding approx. matching
      o new tests:
            BEGIN T1; INSERT K1; ERASE K1; FIND K1; -> error
            BEGIN T1; INSERT K1; ERASE K1; FIND K1 in T2; -> conflict
            BEGIN T1; INSERT K1; ERASE K1; COMMIT T1; FIND K1; -> error
            BEGIN T1; INSERT K1; ERASE K1; COMMIT T1; FIND K1 in T2; -> error

      o create cursors on the stack as well
            o cursors should form a single-linked list

o move template parameters "up" to the LocalDatabase level, or at least
    The goal of this exercise is to
    - reduce the number of virtual calls
    - avoid virtual inheritance of the nodes
    - reduce the number of classes and object instantiations
    - clean up the interfaces
    o Btree or Database? Make a decision
    - however, this requires that all template-code is inline. So maybe start
        with the DB, but do not make all code based on templates, only
        the Btree and some other things (i.e. key/record verification)

o Make Pages resizable with variable length
    Pages can have different sizes, and are always aligned to the "native"
    page size of a Database (with the exception of header pages).

    ==============
    instead of having such a complex solution: would it be enough to have
    different sizes PER database?
    Each database would have the following properties:
      - size for an index page
      - size for a leaf page
      - size for a blob page
      - would this be enough??
    ==============

    These schemes are used for disk-based AND in-memory based databases!
    For this to work, the in-memory device must use a synthetic page ID
    o differentiate between page id and address; the address is only
        used by the Device!
    o in-memory device uses a synthetic page ID (atomic discrete counter)
    o rewrite allocation code path for in-memory pages
    o checkpoint - all in-memory tests must run

    The page IDs are now compressible and have variable length. The first
    2 bits are specifying the compression:
        00 - Page ID embedded in flags and the following byte,
        (2b) Page Size is always 1
             max index size: 14 bits - 268 mib
        01 - Page ID embedded in flags and the following 2 bytes,
        (4b) Page Size: 1 byte
             max index size: 22 bits ~ 68 gib
        10 - Page ID embedded in flags and the following 3 bytes,
        (6b) Page Size: 2 byte
             max index size: 30 bits ~ 17 tib

    o deprecate the HAM_PARAM_PAGE_SIZE parameter for the Environment
    o use HAM_PARAM_PAGE_SIZE parameter for the Database
    o each page needs to be aware of its own size
        o don't forget the journal!
    o store the size persistently
    o print in ham_info
    o adjust ham_bench (the monster tests should not require any changes)

    o rewrite pickle.h
        o create code to read and write the page ID (a special type)
        o page IDs are addressed as uint8_t *, not uint64_t
        o create code to read and write the blob ID
        o blob IDs are addressed as uint8_t *, not uint64_t
    o make sure that PageManager still works (the only user of pickle.h)
    o use compressed page id for root page storage

    o The InternalRecordList and the DefaultRecordLists need to use blocks
        to efficiently manage variable-length IDs
        o use the new code in the btree, but try to stick to uint64_t for
            the external interface! this way we reduce the changes to a small
            part of the code
    o also use this for storing duplicates!
    o run monster tests

    o if no page size is specified: come up with good default values for
        blob pages and indices and leafs
        o make blob pages larger than normal pages
        o make index pages larger if duplicates are used

    o create code to write *groups* of PageIds and BlobIds; these groups
        will be used for the record lists (also for the duplicates)

    o remove the get_record_id()/set_record_id() methods from the record list
        interface (can be implemented by the BtreeNodeProxy base class)
    o combine BaseRecordList and BaseKeyList in a common base class

    o create code to write *groups* of PageIds and BlobIds; these groups
        will be used for the record lists (also for the duplicates)

    o remove the get_record_id()/set_record_id() methods from the record list
        interface (can be implemented by the BtreeNodeProxy base class)
    o combine BaseRecordList and BaseKeyList in a common base class

    o The InternalRecordList and the DefaultRecordLists need to use blocks
        to efficiently manage variable-length IDs
    o also use this for storing duplicates!

    o run monster tests
    o run performance tests

    Instead of splitting pages, they can now be resized easily. This will help
    when applying BtreeUpdate. Instead of requiring a split (and a recursive
    descent of the tree), the BtreeUpdate are applied to a "bigger" page. The
    original page will be obsolete, pointing to the new page, but still contain
    valid data (including the BtreeUpdate). As soon as the parent pointer
    and the sibling pointers are updated, the original page can be
    garbage-collected.

    The actual update flow is like this:
        1. (the original page is read-locked)
        2. a new, larger page is allocated
        3. BUs are merged into the new page
        4. the old page's state is "obsolete", with a pointer to the new page
        5. (the old page's read-lock is released)
    All other changes can be handled separately (updating pointers etc).

o always use constant sizes for the header page(s)
    o 1kb for the EnvironmentHeader
    o 4kb for the PageManager state
    o 11kb for the list of databases
    o improve logic in ham_env_open
    o remove dependeny to header page in the PageManager; it now has its
        own header page!
    o clean up EnvironmentHeader, header page in LocalEnvironment
    o clean up BtreeHeader etc

o Do not add read-only pages to the Changeset
    There's no need to add pages to the Changeset if they are not modified.
    My first thought was that the lock should be replaced with a RW-lock,
    and such operations (i.e. get_record_size(), find(), purger etc) acquire a
    read-lock. But the current implementation does not require this level of
    concurrency. Even if concurrency is implemented on a database-level
    then there's no need for RW-locks in the pages (as long as blob pages
    are NOT shared between databases!)

o clean up BtreeIndex class; it has too many responsibilities, i.e.
    managing configuration, persisting configuration, btree traversal,
    btree actions etc
    o the configuration of all btrees is moved to a separate class
    o the BtreeIndexFactory can create or open btree indices; it manages
        the page with the persistent configurations
    o the BtreeIndex has a persistent configuration, a runtime configuration
        (DatabaseConfiguration) and entry points for btree actions
    o create a common root class for BtreeActions with common functions,
        i.e. traversal

o BtreeUpdate managed in the BtreeNode
    BtreeUpdate are attached to a node. A background thread can merge them.
    At runtime, they're consolidated, and queries (and other operations)
    can work directly on the lists.
    A DeltaUpdate can be part of a Transaction.
    The existing TransactionOperation structure is replaced. The whole
    Transaction handling will be rewritten. Instead of using separate
    TransactionTrees, the BtreeUpdate are attached to the node.

    BtreeUpdate are a sorted (!) linked list. Prior to merge, those
    BtreeUpdate that delete keys will be applied.

    The DeltaUpdate structure should be parameterized, just like the Btree
    (i.e. use the identical type and compare function).

    Pages are not merged or split as long as BUs are attached. Instead of a
    split the page is grown and moved.

    The DeltaUpdate structure has a pointer to its Transaction. If the
    Transaction is committed or aborted, then the DeltaUpdate state is set to
    "committed" (or "aborted"). When merging, aborted BtreeUpdate are removed.
    The Transaction pointer is only followed if the state of the Transaction is
    not clear (i.e. either committed or aborted). 
       
    For recovery, only few changes are required. All BtreeUpdates are
    written to the journal. Each has its own lsn, and each page stores the
    lsn of the last merged BU. Whenever a transaction is committed
    (and flushed), all affected pages are flushed. When that is done,
    the journal file can be switched just as it is done now.

    During recovery, BUs are appended to their pages. No flushes/commits
    are necessary. It is also not necessary to re-open the journal.

    o create a DeltaUpdate struct
        o template parmeters for key and compare function
        o key, record, ...
        o the state (committed, aborted)
        o pointer to the Transaction
        o single "next" pointer for the linked list

    o define an auto-merge strategy
        o in the beginning, BUs will always be merged when the node is
            accessed for reading or most other operations. This will change
            as soon as data can be consolidated on the fly.
        o what if a split is required during auto-merge?
            - first vacuumize (this happens automatically)
            - either call ham_db_insert() to traverse the tree
            - or split into a sibling page, insert the page in the leaf but add
                the parent link later on (queries must make sure that they check
                the sibling node), then fix the parent node later
            - or don't split at all, but resize the page. Use logical page Ids
                and simply switch the page address of the original page,
                then split the page asynchronously
        - if the Database is closed: write all BUs to a log and re-apply
            them when the database is opened

    o Each BtreeNodeProxy manages a (single) linked list of BtreeUpdate
        o but also keep track of the tail for fast append operations
    o This linked list is always sorted
    o Add a flag to force-flush immediately (disable queueing) - per database!
        -> this is a counter which specifies the queue length till the queue
            is merged. default is max<int>, 0 disables queueing
        o support this in ham_bench

    o Merge the updates (naive implementation: apply one by one)
        o first apply "delete" operations
        o then all others
        o ... before flushing the page
        o ... before all other operations
    o When splitting the page: re-distribute the updates to the
        new node
    o Don't merge pages as long as delta-updates are attached
    o implementation checkpoint - all tests must run
        o unittests
        o monster tests
        o performance tests
    o Then consolidate the updates piece by piece
        o for lookups
        o for inserts
        o for erase
        o for traversal
        o for cursors
        o for scans
        o ... and with duplicates

    o Replace the TransactionOperation with the BtreeUpdate
        o requires double linked list to all BUs within this txn?
        o txn is committed: set the state of all BUs to "committed"
        o txn is aborted: set the state of all BUs to "aborted"
        o move the consolidation away from the txn layer towards the BU layer

o Refactoring: rewrite the Transaction layer
    o remove the TransactionNode structure, it's no longer required
    o remove the TransactionTree structure, it's no longer required
    o temporary txn's should no longer exist as a structure; their
        BUs are automatically set to "committed"
    o move duplicate key consolidation down to the BU layer
    o the remaining Transaction layer should only manage txn-id and
        txn name; nothing else!
    o the Transactions should no longer form a linked list
    o temporary Transactions should be created on the stack, without any costs

o Refactoring: rewrite the whole cursor layer
    o clean up the public interface
    o remove the Transaction cursor, merge BtreeCursor with LocalCursor
    o there should be 3 states:
        - nil
        - coupled to btree
        - coupled to txn
        o state() - returns the state
        o set_state() - changes the state
    o key() returns the current (coupled) key
    o record() returns the current (coupled) record
    o get rid of the DuplicateCache and the duplicate index in the cursor; if
        the DeltaUpdates are correctly sorted, then the cursor should not be
        necessary (unless the duplicate position is explicitly required via
        ham_cursor_get_duplicate_position())

. More things to refactor in the btree
    o EraseAction uses duplicate_index + 1, InsertAction uses duplicate_index
        -> use a common behaviour/indexing
    o EraseAction line 71: if the node is empty then it should be merged and
        moved to the freelist!

. when splitting and HAM_HINT_APPEND is set, the new page is appended.
    do the same for prepend!

. Refactoring: all unittest fixtures should derive from a BaseFixture,
    which creates an Environment, creates a list of databases (w/ parameters),
    and if required also a cursor, a transaction and a context
    o include additional management functions like lenv(), ldb(), ltxn(),
        page_manager(), cache(), context()...
    o what else?
    o then reorganize the tests
        - public API
        - internal modules

. The PageManager state is currently stored in a compressed encoding, but
    it is less efficient than the standard varbyte encoding because
    pages > 15 * page_size have to be split. Use a standard vbyte encoding
    instead (it will anyway be required later on).





o Concurrency: merge BtreeUpdates in the background

o Refactoring: would it make sense if each Database has its own BlobManager?
    Then ham_env_erase_db would be much faster, and if each Database has its
    own lock then the blob pages would not block each other
    o are there other modules that should be "per database" and not
        "per environment"?
        - PageManager
        - Device
        - ...?

o LocalEnvironment::open creates a "fakepage" to find out the page_size;
    just assume the default page size of 16kb and read the first page. Then
    discard if the real page_size is a different one
    o deal with databases < 16kb?

o migrate to libuv 1.0, it has a stable API
    http://docs.libuv.org/en/latest/migration_010_100.html
    o also for windows!

o improve the webpage documentation
    o document the various btree formats on the webpage, with images
        o variable length keys (w/ extended keys)
        o POD keys
        o default records
        o inline records
        o fixed-length records
        o duplicates (w/ overflow tables)
        o PRO: compressed keys
        o PRO: compressed records

. investigate "pointer swizzling": internal btree nodes should store "hints"
    to the actual Page, not the Page IDs, as soon as the Page was loaded for
    the first time. This could circumvent the buffer cache and increase
    performance.
    How to invalidate those hints or discover that a page was evicted from
    cache?
    - Eviction could only free the persistent part of a page, and not the
        stub.
    - Could also use reference counting for the page





o PRO: Group Varint-related improvements
    o will currently not work with pages > 16kb
    o vacuumize (internal == false): merge two blocks if they're underfilled?
        we have to reduce the number of indices
        o also for varbyte?

o PRO: allow compression of 32bit record numbers

o PRO: use zint32 compression for internal nodes
    -> requires 32bit page IDs

o PRO: prefix compression for variable-length keys
    use an indirection for the prefixes and suffixes; store each
    part in a slot. the keys themselves have then fixed length (2 slot id's)
        ==> supports efficient binary search!
        ==> is efficient for random read/writes AND linear scans
        however, it's very complex to figure out how to optimally split the
        strings into prefixes and suffixes
    ==> prefixes and suffixes can be stored as extended keys if they become
        too large
    see indexcompression2009.pdf - Efficient index compression in DB2 LUW
    o look for more research papers

o PRO: look for a better compression for DefaultRecordList, i.e.
    - Each group is a GroupedVarInt w/ 4 bits per entry; a 64bit
        number can then hold flags for 16 numbers
        -> (but maybe increase this to hold at least 32 or 64 numbers, to
            reduce the overhead ratio)
    o create a GroupedVarInt<Max, T> class, where |Max| is the maximum number
        of elements that are grouped, and T is the type of these elements
        (i.e. uint64_t)
        -> memory is managed by the caller
        -> the state (i.e. used block size etc) is stored externally, and
            managed by the caller
        o append a key
        o prepend a key
        o insert a key in the middle
        o grow blocks
        o split blocks
        o can perform copies w/o re-compressing

    o try to move the Zint32 index to a base class
    o Use small index which stores offset + bits for each group
    o a separate bit is used to signal whether the (compressed) number is
        a record id
    o avoid ripple effects by growing/splitting the block

o PRO: use compression also for duplicate records
    i.e. use GroupedVarint for inline duplicates








. hola - next steps
    o support java api
    o support .net api
    o support erlang api
    o lua-functions as callbacks - then remote marshalling will work
    o PRO: compile callbacks with clang remotely
    o add remote support where it makes sense (only for PRO?)

. architecture for a new webpage
    o pick an awesome design
        i.e. similar to http://foundationdb.com, http://laravel.com,
        http://rethinkdb.com, http://www.orientechnologies.com
    o use make/m4/markdown to generate static pages:
        https://github.com/datagrok/m4-bakery
        https://developer.github.com/v3/markdown/
    o come up with the full site structure/contents
        http://sidekiq.org/pro/
        o include full documentation, one page per API
        o ... and for all languages
        o keep the documentation in the source tree, not in -www?
    o documentation comments are hosted on disqus
    o blog comments are hosted on disqus, articles are also written in markup

    o Makefile can "scp -r" everything to the servers (staging or production)

    . client area with (low priority)
        o authentication
        o collection of files
        o analytics (who downloads what and when?)
    . admin area with (even lower priority)
        o authentication
        o customer database
        o implementing business processes
        o sending out release emails
        o importing new releases
        o etc


. hola: use sum-prefix-trees to precalculate partial sums/results?
    they could be stored in a btree, and used to dynamically recalculate
    requested values 
    https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf

o QuickCheck: automatically test the recovery feature by invoking "crashes"

o QuickCheck: create a new property for testing duplicates; the key is
    always the same. The number of duplicate keys is tracked and
    periodically checked with the API. A cursor can be used to remove a
    specific duplicate, or to fetch a specific duplicate.


. use cache-oblivious b-tree layout
    -> http://supertech.csail.mit.edu/cacheObliviousBTree.html
    o see roadmap document for more information
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
        (but what about the blobs??)
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    o after resize: mmap the whole file area. this is actually important because
        mmap is much faster than r/w; but when the database is created, the
        original mapping already exists. therefore we might have to handle
        more than one mapping in the file
    o PageManager: when allocating a new page then use the distribution
        function to fetch a page from the reserved storage
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)
        -> could create a second memory mapping for the next chunk

o PRO: hot backups (vacuumizes to a different file)
    really only for PRO?
    http://sqlite.org/c3ref/backup_finish.html
    - make sure that all transactions are closed
    - perform ham_env_flush
    - then copy the file
    - if compaction is enabled: copies keys w/ iterator
        (later: performs bulk updates)
    --> think this through; how to deal with delta updates? -> merge them
        what if only a few databases should be backed up?
        what if i want to back up in a logical format (i.e. csv)?

o "hola" - olap functions that operate directly on the btree data
    -> see wiki
    -> see java8 stream API:
        http://download.java.net/jdk8/docs/api/java/util/stream/Stream.html
    -> see supersonic:
        https://code.google.com/p/supersonic/
    -> see fast bitmap indices
        http://code.google.com/p/lemurbitmapindex/
    o create a design
    o operations on compressed data (COUNT(), MIN(), MAX(), ...)?
    o use async operations or futures/promises
    o deprecate ham_db_get_key_count() (tutorial, documentation)

- bloom filter -> PRO
- concurrency -> PRO

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

. PRO: should we have a separate "recsize == 0" RecordList for duplicates?
    they could only store the duplicate count (but should be able to deal
    with duplicates that are > 256!)
    -> requires grouped varints

o asynchronous prefetching of pages
    -> see posix_fadvise, libprefetch

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

