I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.2.0-pre1 (in decreasing priority)
x refactoring of configure.ac -> topic/next
x refactoring of the file format -> topic/next
x make freelist statistics endian clean -> topic/next
o btree understands fixed-size PODs -> topic/next
    o support existing callbacks - everything should work as advertised
    o support binary search only
    o support POD types: int32, uint32, int64, uint64, float, double,
        fixed length blobs, fixed length strings (?)
        (w/o duplicates and extended keys - they always enforce the
        default type based on the callback)
o cache oblivious btree layout (if it makes sense!) -> COMM
o specify timeout for remote client
o monster test can create graphs

-----------------------------------------------------------------------------

x rewrite configure.in; look at libuv for a good sample
        x also for bootstrap.sh
    x rename to configure.ac
    x test on MacOS (tcmalloc must be disabled!)
    x also for monster tests

x fix java compiler warnings

x collect file format incompatibilities
    -> see topic/next

o specify timeout for remote client
    x add new option HAM_PARAM_NETWORK_TIMEOUT_SEC
    x for unix
        struct timeval tv;
        tv.tv_sec = sec;  // 30 Secs Timeout
        tv.tv_usec = 0;
        setsockopt(sockfd, SOL_SOCKET, SO_RCVTIMEO, (char *)&tv, sizeof(tv));
    x for java
    x for dotnet
    x needs a unittest
    x test on MacOS
    o for Windows
        http://msdn.microsoft.com/en-us/library/windows/desktop/ms740476%28v=vs.85%29.aspx
<<<<<<< HEAD
=======
    o needs a unittest
    o for java
    o for dotnet
<<<<<<< HEAD
=======
=======

high-level plan for 2.1.3
- refactoring of configure.ac
- refactoring of the file format
- btree understands fixed-size PODs
- cache oblivious btree layout (if it makes sense!) -> COMM
- specify timeout for remote client
- monster test can create graphs
=======
>>>>>>> Removed file format cruft, incremented file version

high-level plan for 2.1.3 (in decreasing priority)
x refactoring of configure.ac
x refactoring of the file format
o make freelist statistics endian clean
o btree understands fixed-size PODs
    o support existing callbacks - everything should work as advertised
    o support binary search only
    o support POD types: int32, uint32, int64, uint64, float, double,
        fixed length blobs, fixed length strings (?)
        (w/o duplicates and extended keys - they always enforce the
        default type based on the callback)
o cache oblivious btree layout (if it makes sense!) -> COMM
o specify timeout for remote client
o monster test can create graphs
>>>>>>> Darwin: do not use tcmalloc

-----------------------------------------------------------------------------

x collect file format incompatibilities
    x persistent freelist statistics
    x persistent freelist payload
    x page header - reserve space for 32bit crc, 32bit page size
    x for cache-oblivious (pre-allocated) index
        BtreeHeader:
          x start offset of reserved area
          x number of reserved pages
    x for the new Btree code (btree node, database header)
        BtreeHeader:
          x 32 bit flags (fixed size, var size)
          x key size
          x record size (reserved)
        BtreeNode:
          x 32 bit (!) flags
          x 32 bit (!) counter
    x make sure that the file format is extensible for more features while
        still being backwards compatible (but not forward compatible)
        -> should be ok
    x increment file format counter and make sure that older database files
        cannot be read by 2.1.3 (and that 2.1.2 fails to read the newer
        database files)
        x add unittest: load file with version 0 (Hamsterdb/openVersion1.x)
    x document this! Also document the upgrade process.

x rewrite configure.in; look at libuv for a good sample
        x also for bootstrap.sh
    x rename to configure.ac
    x test on MacOS (tcmalloc must be disabled!)
    x also for monster tests

o release-v2.pl enhancements
    x needs to run with enable-gcc-hardening tests
    x needs to run valgrind tests
    o needs to run recovery tests
    o needs to run with static code analysis (clang)
    o new parameter "--start=3" in combination with "run": starts at
        step #3, executes all following steps
        "--stop=5": stops at step #5

o remove libjson, use boost::property_tree instead!

. review/rewrite/refactor Transaction class
    keep in mind that sooner or later the BtreeNode will expect template
    arguments; can we do something similar with the TransactionNode?
    x try to get the methods/design analoguous to the Btree - no, that
        does not make too much sense
    x document the tree structure in the header file
    x refactor the code
    o split into multiple files
    o try to cleanup the flow; move more code from db.cc to txn.cc
    . each Transaction should have its own PoolAllocator (based on ByteArray);
        when deleting the whole structure, only the PoolAllocator is freed
        o only do this if memory allocations cost performance
        o problem: a realloc() will not always work because it might move
            the allocated memory, and all existing pointers would be invalid 

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

o collect file format incompatibilities
    o for the new Btree code (btree node, database header)
    o persistent freelist statistics
    o persistent freelist payload
    o page header - also store crc, page size
    o get rid of the statistics-structures
    o reduce the PBlobHeader (remove flags?, alloc_size)
    o for cache-oblivious (pre-allocated) index
    o what else?
    
    o increment file format counter and make sure that older database files
        can not be read by 2.1.3
>>>>>>> Fixing valgrind errors and leaks
=======
o persistent freelist statistics are not endian-clean!
>>>>>>> Removed file format cruft, incremented file version

o prepare btree rewrite; the goal is to move all node-specific operations
    into the node itself, and ultimately to get rid of BtreeIndex::compare

    o analyze node; move all node operations to the node itself
        o PBtreeNode::get_key()
        o find(key) -> position || kNotFound (== -1)
            -> BtreeIndex::get_slot
               (rename to get_position_in_page; returns position or kNotFound)
        o insert(key, RecordIdProxy) -> Proxy creates RecordId if insert is
                successful
            -> needs_split() -> bool
            -> split(Page *newpage, int pivot, bool internal)
            -> insert(key, record_proxy, flags = HINT_PREPEND, HINT_APPEND,
                        OVERWRITE, DUPLICATE) -> {slot, duplicate_id}
            -> append_no_split(key, record_proxy) -> {slot, duplicate_id}
            -> prepend_no_split(key, record_proxy) -> {slot, duplicate_id}
        o erase
            o remove_entry(Page, slot) -> erase_from_page(Page, slot)
            o copy_key(SrcPage, SrcSlot, DestPage, DestSlot)
            o replace_key(SrcPage, SrcSlot, DestPage, DestSlot)
            o shift
            o merge
        o what else?
            o Node::initialize() - called when a new page is allocated
            o need an "iterator" concept where it is possible to get the
              previous or the next key (for approx. matching and cursors)
            o need to change the PBtreeKey class - it might not have a size!
    o implement new code flow
        o Abstract baseclass BtreeNodeProxy, which is then derived and
            implemented with template parameters. The BtreeNodeProxy will
            have all the additional logic for the PBtreeNode (which will
            not be modified).
        o Need a factory for BtreeNodeProxy objects; The generated pointer is
            stored in the Page object (make sure it's deleted when the page
            is moved to the freelist!)
        o The factory creates a BtreeNodeProxy which is identical to the
            current implementation (and which uses a template object with
            the callback function from the LocalDatabase, and a
            template parameter for the key proxy)
        o The default btree Comparator should refer to the Database callback
            function
        o Start by rewriting BtreeCheckIntegrityAction
        o Then rewrite BtreeEnumAction
            o rename to BtreeScanAction
        o BtreeInsertAction
        o BtreeFindAction
        o BtreeEraseAction
        o ... what else?
        o Introduce HAM_PARAM_KEY_TYPE == HAM_TYPE_UINT32 (= "u32") and create
            a new BtreeNodeProxy in the factory; add a new comparator and
            reimplement everything for a 32bit integer
        o Create all other POD types

         o After the implementation, the following rules apply:
            o PBtreeNode::m_entries needs to be removed
            o The database compare callback will not be used if the POD type
                was selected
            o PBtreeKey is gone (more or less)
            o The Btree code is a low-level module, ONLY accessing
                o Utils
                o Error
                o PageManager
                everything else must be handled through implementation classes
                or functors provided by the Database (i.e. access to
                duplicate manager, blob manager etc).
            o What else?

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

. specify timeout for remote client
    add new option HAM_PARAM_NETWORK_TIMEOUT_MSEC
    o needs a unittest
    o for unix
        struct timeval tv;
        tv.tv_sec = 30;  // 30 Secs Timeout
        tv.tv_usec = 0;
        setsockopt(sockfd, SOL_SOCKET, SO_RCVTIMEO, (char *)&tv, sizeof(tv));
    o for Windows
        http://msdn.microsoft.com/en-us/library/windows/desktop/ms740476%28v=vs.85%29.aspx

. monster tests must create performance graphs, namely
    o insert latency
    o erase latency
    o lookup latency
    o also for bdb!
    use https://developers.google.com/chart/interactive/docs/gallery/linechart
    and generate a small html file with the graphs, then open it in the browser
    (if possible).
    The Y axis is the latency (the lower the better), the X axis is a time
    indicator (not a timestamp, but a chronologically increasing counter).
    Also, there should be tooltips with the key size and record size
    of this particular insert or erase or lookup.

. it would be interesting to know the distribution of changelog "sizes" during
    a test (i.e. how many operations result in changelogs of size 1, 2, 3 etc)
    and how many operations actually require the WAL



------------------- release 2.1.3 -----------------------------------------

high-level plan for 2.1.4
- btree understands variable-sized PODs
- completely rewrite handling of extended keys
- btree can compress keys and records -> COMM
- use SIMD for scans (if it makes sense!) -> COMM
- replace 3rdparty/json with boost property_tree

high-level plan for 2.1.5
- completely rewrite handling of duplicate keys
- asynchronous read-ahead (if it makes sense!) -> COMM
- combine POD types into high-level schemas (split off something for COMM!)
- lay the groundwork for concurrency 

o continue with the Btree rewrite
    o support linear search through a node (with skip list?)
    o support variable length types, use linear search (with a skiplist)
        for strings (strcmp), blobs (memcmp)
    o replace extended keys; store them in the leaf unless they're TOO big
        (then either use an overflow area or refuse to store them i.e. if
        they are > 20% of the page - better refuse, then we can also get
        rid of the error code that can be returned by the compare function)
        -- really? wouldn't this be inconvenient for the users?
    ------- release -------
    o replace duplicate keys; this will be difficult because it requires
        rewriting the Cursor consolidation flow
    ------- release -------
    o optionally store fixed length record in leaf (not in internal pages!)
    ------- release -------
    o add column store compression for keys, lightweight compression for
        records (snappy, etc) -> COMM!
    ------- release -------
    o add "functions" and predicate scans, i.e. COUNT() w/ predicates???
    ------- release -------
    o add combined high-level schemas (combinations of various POD types)
>>>>>>> Fixing valgrind errors and leaks
>>>>>>> Removed file format cruft, incremented file version

o win32: a few unimportant unittests are failing (2.1.2)

o search hamsterdb.cc for this: "re-enable this after 2.1.1, when the"...
    and fix it

o monster tests must create performance graphs, namely
    o insert latency
    o erase latency
    o lookup latency
    o also for bdb!
    o also be able to include information from a previous release
    o use an embedded server in perftest.pl
        (i.e. http://search.cpan.org/~jesse/HTTP-Server-Simple-0.44/lib/HTTP/Server/Simple.pm)
    use https://developers.google.com/chart/interactive/docs/gallery/linechart
    and generate a small html file with the graphs, then open it in the browser
    (if possible).
    The Y axis is the latency (the lower the better), the X axis is a time
    indicator (not a timestamp, but a chronologically increasing counter).
    Also, there should be tooltips with the key size and record size
    of this particular insert or erase or lookup.

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

o increase libtool version
o increase API version in header file (if there are any incompatibilities)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. it would be interesting to know the distribution of changelog "sizes" during
    a test (i.e. how many operations result in changelogs of size 1, 2, 3 etc)
    and how many operations actually require the WAL

. review/rewrite/refactor Transaction class
    keep in mind that sooner or later the BtreeNode will expect template
    arguments; can we do something similar with the TransactionNode?
    x try to get the methods/design analoguous to the Btree - no, that
        does not make too much sense
    x document the tree structure in the header file
    x refactor the code
    o split into multiple files
    o try to cleanup the flow; move more code from db.cc to txn.cc
    . each Transaction should have its own PoolAllocator (based on ByteArray);
        when deleting the whole structure, only the PoolAllocator is freed
        o only do this if memory allocations cost performance
        o problem: a realloc() will not always work because it might move
            the allocated memory, and all existing pointers would be invalid 
. also remove locking from C# and Java APIs

. BtreeCursor: use memory arena for uncoupling the key
    -> better wait till extended keys are gone

. also remove locking from C# and Java APIs






------------------- idea soup ---------------------------------------------

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o flush transactions in background (when the btree is concurrent)

. remove libjson, use boost::property_tree instead!
    o also on Windows!

o Improve leaf pages caching
    Store start/end key of each leaf page in a separate lookup table in order
    to avoid btree traversals. This could be part of the hinter.
  - one such cache per database
  - should work for insert/find/erase

o allow transactions w/o journal

o allow transactions w/o recovery

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o when flushing the Changeset: batch ALL changes for the WHOLE transaction,
    then flush all of them together. This way we can "merge" multiple changes
    for the same page.
    Also review the whole flush process - when not to log etc.
    - only 1 page affected: no need to log it because it is idempotent
    - freelist pages are always idempotent
    - more than 1 index page? not idempotent (most likely)
    - more than 1 blob page? not idempotent (maybe)
    o define a few benchmarks
    o be careful: if N operations are modifying the same changelog, and
        then #N+1 aborts then the aborting operation must NOT clear the
        changelog!

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o changeset: instead of simply adding pages to the changeset, the caller
    could already specify whether this page needs logging or not;
    i.e. after freelist rewrite, the blob pages do not need logging if a
    blob is deleted  

o is there a way to group all changeset flushes of a Transaction into one
    changeset, and batch-commit multiple commits? that way we would avoid the
    frequent syncs and performance would be improved
    o would have to remove all of assert(changeset.is_empty())
    o but we can use that assert prior to txn_begin

o BtreeFindAction: always use a cursor, and when doing approx matching
    then simply move left or right with that cursor

o flush in background (asynchronously)
    o need new flag file HAM_DISABLE_ASYNCHRONOUS_FLUSH
    o if in-memory database: disable async flush
    o if transactions are disabled: disable async flush
    o if enabled: create background thread, wait for signal
    o ham_env_flush: if txn are enabled then try to flush them to disk
    o how to deal with an error in the background thread???
        o store in Environment, then return in every exported function
    o default: async flush is OFF!

    o extend monster tests
        o with async flush
        o without async flush
        o extend/run performance test
        o run monster tests

    o documentation
        o tutorial
        o faq

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    -> or: make this the default; call the new flag HAM_TXN_MAYBE_WILL_ABORT
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o ham_get_count: could be atomically updated with every journal entry

