
o benchmark 
    100mb-file mit memmapped access, random
    dann die selben zugriffssequenzen mit read/write
    random access, aber eine gewisse lokalität sollte vorhanden sein (wie es
    auch beim btree der fall ist)

o refactoring
    o sauber getrenntes device-subsystem: create, open, read, write, close,
        alloc, free; nimmt mmap, wenn möglich, sonst read/write; ausserdem
        prefetch mit async io
    o interface von device-subsystem sauber definieren, darauf aufbauend
        den page manager (ersetzt den cache), der die txn als 
        parameter bekommt (kann 0 sein). die txn ist keine schicht oberhalb
        des pm!
    o ham_page_t braucht persistent header, mit flags, page-typ und anzahl
        der elemente; die leaf-seiten werden anders gehandhabt, können 
        einen grösseren datenteil haben als internal pages (haben dadurch
        weniger elemente).
    o freelist rausziehen aus der header-page in eigenes modul; freelist-
        operationen bekommen weiterhin den txn-parameter (kann 0 sein)

---------------------------

x was ist die default pagesize?  -> 4kb

o profilen
    x tests nur mit inserts - verhältnis zu bsddb?
        -> test insert.tst
            hamster:  0.343sec
            berkeley: 0.270sec
    x tests nur mit erase - verhältnis zu bsddb?
        -> test 45.tst
            hamster:  0.318sec
            berkeley: 0.229sec
    o testen mit time
        berkeley (45.tst)
            real    0m0.099s
            user    0m0.035s
            sys     0m0.026s
        hamster (45.tst)
            real    0m1.082s
            user    0m0.944s
            sys     0m0.100s
    x tests mit create/close - verhältnis zu bsddb?
    o statistiken in hamster einbauen
        o cache hits
        o cache misses
        o flushes
        o allocs
        o frees
        o freelist-hit
        o freelist-miss
        o freelist-add
    o profiling - gprof
    o mit strace prüfen, wann berkeley wirklich plattenzugriffe macht!
        default cachesize/pagesize an berkeley anpassen

o beim einfügen von werten mit sizeof(wert)<=sizeof(ham_offset_t)
    können wir die werte direkt im leaf speichern. aber wie erkennen wir
    das? wir müssen noch ein flag im key mitspeichern (1 byte). 
    default key grösse anpassen!
    flag & DATA_IN_OFFSET: data hat 8 byte, ist im offset
    flag & SMALL_DATA_IN_OFFSET: data hat <8 byte, ist im offset; an byte 0
        steht die echte grösse

o wie verhält sich der default-comparator (bei ungleichen keygrössen)?
    1. memcmp auf prefix
    2. identisch? wenn ein key kürzer ist als der andere, und vollständig
        vorliegt: kurzer key ist "vorher"
    3. sonst keys nachladen, wie bei 2) weiter

o variable length keys
    o grösse prüfen: nach 8 stunden hat hamster 121 MB, berk 64 MB
        o wird die freelist korrekt benutzt?
        o werden data blobs immer alignt?
        o wie wiele pages hat berkeley für den index, wie viele hamster?
    o testen mit verschiedenen cachegrössen
        o wird die grösse eingehalten?
        o cachegrösse 0?
        o readthrough/writethrough
    o testen, testen, testen
        o variable length keys
        o non-numeric keys
        o sehr grosse datensätze (bis 100 MB)

o beim schliessen der db sollten alle txn geschlossen werden

o sicherstellen dass in der aktuellen version immer nur 1 transaktion 
    geöffnet ist!

-------------- -------------- -------------- --------------

o btree: address of root-page must be part of the (persistent) backend

o support multiple indices
    max. number of indices can be set at compile time (default: 16 or 32 - 
    check sizes!)

o db_get_keysize() -> backend_get_keysize()

o statistics
    get generic getter/setter for statistical value, i.e. 
        enum { CACHEHITS, CACHEMISSES, PAGES, FREEPAGES, ... };
        ham_u64_t statistics[8];
    set_stats_value(db, which, value): db->statistics[which]=ham_h2db32(value)
    ATTENTION: some of those values are specific for each backend, others
        are global; therefore we need two functions: 
        ham_get_global_statistics(stat)
        ham_get_index_statistics(index, stat)

--------------

o configuration management: HAM_32BIT/HAM_64BIT, HAM_OS_POSIX/HAM_OS_WINDOWS
    automatisch während des build-prozesses erkennen; momentan 
    werden sie in ham/config.h gesetzt

o stress test mit zufällig fehlschlagenden io-funktionen und zufälligen
    OUT_OF_MEMORYs

--------------

o review: make sure that error codes are propagated correctly (ham_set_error())

o new flag for create/open: USE_FLOCK calls flock() around insert/erase 
        (makes mostly sense if cachesize==0)

o profiling: if a page is allocated, allocate more pages, then we have only
    one disk access

--------------

o documentation -> texinfo?? 
    merge the documentation with doxygen-docs

x use scons instead of make

o define HAM_EXPORT DECLSPEC _dllexport
    compile a shared library

--------------
--------------

o functionen wie page_ref_* im release-mode durch makros ersetzen,
    im debug-mode durch funktionen mit debug-info (__FILE__, __LINE__)

o cache priority:
    beim "delete" von unused pages hat jede page eine priorität. 
    z.b. Index: 20, Data-Header 10, Data 0
    in der ring-liste wird zum letzten Element gesprungen. Dann wird 
    bis zum Start gegangen. Alle Prioritäten der pages werden um 1 
    decrementiert. Wird eine page mit Prio 0 gefunden, wird sie gelöscht.
    Ansonsten wird die Page mit der niedrigsten Prio gelöscht.

o bei btree_find und blob_read macht es derzeit keinen sinn, die 
    seiten in der pagelist zu cachen (ausser evtl die seiten mit dem 
    blob_t header und die seiten, die nur zum teil benötigt werden).

o cache policy
    es gibt
    - strikt: wenn speicher aufgebraucht wurde, gibt's EWOULDBLOCK
    - lax: kurzfristig darf speicher dazuallokiert werden (default)

o test ham_open() - could be part of btree_row; after the db is closed, 
    it's re-opened, and all items are searched with "find"

o duplicate keys
    jeder key bekommt einen dupcounter; der wird mit unsigned(-1) initialisiert
    beim einfuegen wird rekursiv abgestiegen. im leaf wird dann ein neuer key
    eingefuegt, mit dem dupcounter=existierender ("kleinster") key -1.

o in-memory database

o cache verbessern
    o cachesize kann noch nicht geaendert werden
    o moegliche policies: so wenig speicher wie noetig/kein cache, 
        fixe cache size, so viel wie beliebt

o verbessern der key-compares
    zweistufige abfrage:
        int compare_key(const void *lhs, int lhs_size, lhs_fullsize,
                        const void *rhs, int rhs_size, rhs_fullsize);
        rückgabe: -1: lhs < rhs
        rückgabe: +1: lhs > rhs
        rückgabe:  0: lhs== rhs
        rückgabe: 99: request fulldata bei varkeys
    danach wird compare_key() erneut aufgerufen, aber mit den vollständigen
    keys

o prefetch pages, i.e. with async. i/o

o cursors
    @@@

o filter
    generische filter, sowie 2 implementationen davon: ein encryption-codec 
    und ein zlib-codec
    arbeiten sie page- oder blob-basiert? evtl waere page-basiert besser, 
    denn dann koennten auch index-pages (bzw NUR index-pages) gefiltert 
    werden.
    (my_result_t) (*filter)(my_key_t *key, my_data_t *data, int direction);

o hash-tabelle
    @@@

o bindings 
    c++-wrapper (ähnlich stl? müsste möglich sein, aber
    schwer), python-db-modul, perl, java (alle swig?)

o asset-tool
    GUI und library für computerspiele

o logging
    @@@

o locking/concurrency
    @@@

o transactions/ACID
    @@@

o real time support (transaction priorities, timeouts...)
    @@@

--------------
--------------

Version 0.1.0

o bbaum fehlerfrei
o duplicate keys
o variable key length
o caching
o multiple indices
o optionales file locking mit flock vor insert/erase
o live reorganisation
o in memory-datenbanken
o iteratoren
o komplettes review
    werden fehlerwerte korrekt propagiert?
    machen fehlerwerte immer sinn? z.b. in flush_all()
    was passiert mit fehlern beim flushen?
    was passiert mit fehlern in ham_close()?

o stress-test-tool(s), das ALLE optionen durchprobiert
o admin-tool(s) fuer dump, stats, reorg
    o db_dump: 
        o key/daten dumpen machen wir erst später, wenn's iteratoren gibt
        o header dumpen

--------------

Version 0.1.0

o hash-tabelle

-------------------------- literature -----------------------
pB+ Trees prefetching B+ Trees 
[CGM01] Improving Index Performance through Prefetching. S. Chen, P.B. 
Gibbons, and T.C. Mowry. ACM International Conference on Management
of Data (SIGMOD), Santa Barbara, California, May 2001
[GL01] B-Tree Indexes and CPU Caches. G Graefe and P. Larson. International
Conference on Data Engineering (ICDE), Heidelberg, Germany, April 2001.
[RR00] Making B+ Trees Cache Conscious in Main Memory. J. Rao 
and K.A. Ross. ACM International Conference on Management of Data (SIGMOD),
Dallas, Texas, May 2000.

