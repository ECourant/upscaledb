I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

x win32: get rid of boost_thread

x boost: configure script should make sure that only boost v 1.46 (or newer)
    is accepted
    -> also test with older versions, but 1.33 definitely does not work
    issue #17: https://github.com/cruppstahl/hamsterdb/issues/17

x metrics: add extkey_cache:hits, :misses

x AES encryption
    x create a test program which encrypts/decrypts data (cbc)
        http://saju.net.in/code/misc/openssl_aes.c.txt
    x ./configure: enable aes by default if -lcrypto is available
    x ./configure: add new option "--disable-encryption"
    x move prototype to .cc file
    x create header file documentation; HAM_PARAM_ENCRYPTION_KEY is a
        16byte array with the key
    x ham_env_create: new option HAM_PARAM_ENCRYPTION_KEY enables encryption
        x do not allow if in-memory 
            x unittest
        x disable mmap if encryption is enabled
            x unittest
    x ham_env_open: new option HAM_PARAM_ENCRYPTION_KEY
        x add unittests for create/open, test good and bad keys
        x disable mmap if encryption is enabled
            x unittest
    x disable direct I/O
    x encrypt/decrypt pages
        x cbc-encrypted with page-id as salt
        x need tests w/ reopen, inserts (growing size, 1 - 512 bytes)
    x encrypt/decrypt the log (everything is already padded)
        x do not use salt
        x need tests w/ recovery
        x also test in recovery.pl
    x add to monster tests
    x extend the Windows build


x clean up Database class
    x split into multiple files
    x remove the TODOs
    x refactor, reformat if necessary
    x db_local.cc has many static functions; remove them

x clean up Cursors
    x different definitions of Page::uncouple_cursor,
        BtreeIndex::uncouple_cursor, btree_uncouple_cursors 
        x reduce to one method
        x instead of calling BtreeIndex::uncouple_cursor in PageManager
            (and others), the function could be moved "up" in the call tree
            into the relevant Btree modules
        x same for BtreeIndex::free_page_extkeys
    x clean up Cursor interface
    x Page::add_cursor, Page::remove_cursor:
        x move to BtreeCursor
        x try to merge with couple(), uncouple(); the following functions
            should exist:
            x set_to_nil()
            x get_state() -> nil | coupled | uncoupled
            x uncouple_from_page()
            x couple_to_page()
            x get_coupled_key(&page, &slot, &dupe_id)
            x get_uncoupled_key()

x clean up BtreeCursor - there still are many TODOs

x Environment::get_incremented_lsn should return void

o metrics: memory allocations currently ignore operator new/delete

o Rewrite Cursor::is_nil: fix TODOs, do not allow what==0

o Review Cursor documentation in cursor.h, btree_cursor.h, txn_cursor.h

o run a performance test clang++ vs g++

o improve client/server performance
    currently client/server is 10 times slower than standalone; can we improve
    this, i.e. by using keepalive/open connections?
    o configure.in: remove --diable-server, always use --diable-remote
    o create/define a benchmark (i.e. based on client1.c/server1.c sample)
    o use libuv on server side, get rid of mongoose
        http://nikhilm.github.io/uvbook/networking.html
    o use libuv on client side, get rid of libcurl
    o test build with --disable-remote, add this to the release process
    . use Pickle module, get rid of protocol buffers (or better leave for now?)

o Review Cursor stuff: when reimplementing duplicates, try to get rid of the
    duplicate cache and all the tricky code in the consolidation
    -> how?

o LocalDatabase::cursor_insert (and others) should move the relevant functions
    to Transaction and Cursor; do not directly access btree/txntree/txncursor

o clean up Btree::enumerate interface, use C++ object instead of callback
    function

o clean up Environment class
    o split into multiple files
    o remove the TODOs

o when flushing the Changeset: batch ALL changes for the WHOLE transaction,
    then flush all of them together. This way we can "merge" multiple changes
    for the same page.
    Also review the whole flush process - when not to log etc.
    - only 1 page affected: no need to log it because it is idempotent
    - freelist pages are always idempotent
    - more than 1 index page? not idempotent (most likely)
    - more than 1 blob page? not idempotent (maybe)
    o define a few benchmarks

. reduce the file size if freelist adds page at end of file
    o only do this in ham_env_close and ham_env_flush
    o can be disabled with an (undocumented) flag
    o attention: if a file is truncated then recovery can re-create pages
        that are "outside" of the file boundaries! (we could do this only
        when the log is empty?!)

. pre-allocate index; 2.0.0 had this as an experimental feature
    o see roadmap document for more information
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o needs freelist hints to retrieve the next possible
        free page *adjacent to* the previous index page
    o run a performance test if this is worth the effort
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages

o release-v2.sh
    o test build with disabled AES, add this to the release process

. BtreeCursor: use memory arena for uncoupling the key
    -> postpone till we got rid of extended keys?

------------------- release 2.1.1 -----------------------------------------











o collect file format incompatibilities
    o for the new Btree code
    o persistent freelist statistics
    o get rid of the statistics-structures
    o reduce the PBlobHeader (remove flags, alloc_size)
    o what else?

. also remove locking from C# and Java APIs

. BtreeCursor: use memory arena for uncoupling the key
    -> better wait till extended keys are gone

o split Transaction into local and remote class (really?)
    hamsterdb.cc directly calls into Transaction class instead of Environment
    o create PoolAllocator based on ByteArray
    o keep in mind that sooner or later the BtreeNode will expect
        template arguments; can we do something similar with the
        TransactionNode?
    o try to cleanup the flow; move more code from db.cc to txn.cc,
        reorg TransactionIndex similar to BtreeIndex
    o try to get the methods/design analoguous to the Btree
    o TransactionIndex:
        o TransactionIndex: use std::set<OperationNode> instead of rb.h?? Not
            sure if this is worth the troubles
        x check how/if std::multimap can replace rb.h - yes, seems to work
        o replaces the TransactionTree
        o one TransactionIndex per Database (always created)
        o txn_opnode_t -> TransactionOperation
        o try to move code from db.cc into TransactionIndex, i.e. to check
            conflicts etc
    o try to replace some of the linked lists with boost::intrusive_list
    o document the tree structure in the header file

o split Cursor into local and remote class (really?)
    o hamsterdb: directly call into Cursor class (instead of Database)

o is the recovery working if there's a crash during ham_db_close
    or ham_env_close?

o allow transactions w/o journal

o allow transactions w/o recovery

o move the whole configuration (key sizes, parameters, page size, etc) into a
    separate class which is instantiated by the env

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o fully (!) automize the whole release process for unix; the result (on
    success) are the following files:
    o tar-ball
    o the README
    o the documentation
    o the Changelog
    o the release notes (a template)
    o move monster test to ec2?
    o the output of the monster tests and the performance test
    o windows-packages

o update documentation
    x in header file
    o in the wiki
        o don't forget to list all functions that are currently disabled
            w/ txn's -> sorting dupes, approx. matching, ...
        o transactional behavior/conflicts of duplicate keys
    o in the wiki: start with internal documentation
        o transactions
        o architecture
        o btree
        o journal/log
        o cache
        o I/O
        o unittests
        o cursor(s)
        o monstertests - how to use them?





------------------- idea soup ---------------------------------------------

o changeset: instead of simply adding pages to the changeset, the caller
    could already specify whether this page needs logging or not;
    i.e. after freelist rewrite, the blob pages do not need logging if a
    blob is deleted  

o is there a way to group all changeset flushes of a Transaction into one
    changeset, and batch-commit multiple commits? that way we would avoid the
    frequent syncs and performance would be improved
    o would have to remove all of assert(changeset.is_empty())
    o but we can use that assert prior to txn_begin

o BtreeFindAction: always use a cursor, and when doing approx matching
    then simply move left or right with that cursor

. track additional metrics
    o cache misses
    o cache hits
    o ...

o flush in background (asynchronously)
    o need new flag file HAM_DISABLE_ASYNCHRONOUS_FLUSH
    o if in-memory database: disable async flush
    o if transactions are disabled: disable async flush
    o if enabled: create background thread, wait for signal
    o ham_env_flush: if txn are enabled then try to flush them to disk
    o how to deal with an error in the background thread???
        o store in Environment, then return in every exported function
    o default: async flush is OFF!

    o extend monster tests
        o with async flush
        o without async flush
        o extend/run performance test
        o run monster tests

    o documentation
        o tutorial
        o faq

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o ham_get_count: could be atomically updated with every journal entry

