
x release 0.4.6

x check ham_txn_abort in btree_cursor.c
    this call never appears in hamsterdb.c, but quite frequently in the
    cursor routines; is this a problem?
    x no, but enable ham_txn_abort in hamsterdb.c

x update tutorial
    x duplicate keys
    x add GPL3

x more ideas for unittests
    x unittests: insert NULL/TINY/SMALL as duplicates
    x unittests: insert NULL, then replace with TINY, then with SMALL, 
        then with big, then SMALL, TINY, NULL
    x insert some duplicates, then erase all but the last one; the last
        record must be TINY. is the duplicate table deleted? is the rid
        set to the TINY dupe? -> no, but it's ok for now

x ham_close with flag HAM_AUTO_CLEANUP
    x return DB_NOT_EMPTY if closing db but cursors still exist
        x add documentation in header file
        x unittest
    x close all cursors if HAM_AUTO_CLEANUP is specified
        x unittest
    x change the samples

x ham_env_close with flag HAM_AUTO_CLEANUP
    x return ENV_NOT_EMPTY if closing env but databases still exist
        x unittest
    x close all databases if HAM_AUTO_CLEANUP is specified
        x unittest
    x change the samples

x btree_insert:421 - why is the extkey deleted??

x protect users against uninitialized ham_key_t and ham_record_t structures
    x check if key->flags is 0 or USER_ALLOC
    x set key->_rid to 0
    x check if record->flags is 0 or USER_ALLOC
    x check all other private flags or set them to 0
    x modify the samples

x my_blob_is_small must be dependend of the page size!
    currently it's way too small if the pagesize is 64kb!!
    x test different sizes and pagesizes
    x rewrite as a macro

x move resizing of db_set_record_allocdata() to its own function
    x change the code
    x unittests?

x rewrite freelist
    x keep all freelist pages in a static lookup-table
    x resize table when necessary
    x make sure that unittests and acc.tests are running
    x for testing: ./test  --profile=1 ../../testfiles/1/180.tst --duplicate=1
        --verbose=1 takes 90 seconds (35 insert, 55 erase)
    x is the freelist loaded correctly after each reopen? -> rewrite unittests
    x check filesize after acceptance tests
    x fix memory leak in acceptance test

x key_set_record allocates a blob if record is {0, 0}!!

x improve performance of duplicate keys
    x get some numbers of 0.4.6
    x don't use blob_*-routines to manage the dupe_table
        instead, just modify the cached page and set the dirty-flag
        -> must make sure that the dupe_table doesn't span multiple pages
        -> can use this also for other blobs

x record numbers should not be reused
    currently, if the last record is deleted, and then the database is
    reopened, this record number is reused
    persistently cache the number of elements in the database - we need 
    this information anyway.
    x do NOT add a new function ham_get_count(), because this information
        is risky - if keys are inserted, but the application crashes, 
        the information is wrong
    x if the database is opened and the first recno item is appended AND
        the key already exists: re-read the largest key and set the counter
        again - No, not yet. just return ALREADY_EXISTS -> need reorg-tool
    x unittest

x overwrite inmemory-blob: do not re-allocate the memory if the memory sizes
    are the same

x ham_cursor_get_duplicate_count(ham_cursor_t *cursor,
        ham_size_t *count, int flags=0);
    x blob_duplicate_get_count(dupe_table_id);
    x implement public function
    x add header file documentation
    x unittests
    x add tutorial documentation

x new sample for duplicate keys
    modify sample env1, to create a 1:n relationship between orders
    and customers (or create a new sample)

x btree_node_search_by_key uses linear search, no binary search
    -> optimize this!!

o recno-unittest always overwrites the existing database; change the
    test so the original database is copied, and not modified
    o also test ham_find
    o also test ham_cursor_find
    o also test ham_erase
    o also test ham_cursor_erase
    o also test ham_insert
    o also test ham_cursor_insert

o recno on big endian
    x ham_find/ham_erase/ham_cursor_find: make sure that the parameter size 
        is ok
    x ham_find/ham_erase/ham_cursor_find: translate key to db-endian
        (currently, unittest fails on big endian)
    x other unittests are failing
    o check acceptance tests!
    o enhance endian-create.sh and endian-check.sh

o duplicate items on big endian
    x create unittest
    x check-in le-database (created by the new sample)
    x check-in be-database (created by the new sample)
    o check acceptance tests!
    o enhance endian-create.sh and endian-check.sh

o win32: project for sample db5 is missing
    maybe restructure the solution file? one sln for win32, another for win64?
    i don't like it as it is right now
    o add missing samples env1.c, env2.c

o are we using a good default page size? should we rather use 16kb??




o tests are failing:
    --duplicate=1 --pagesize=65536
      x ../../testfiles/1/201.tst - freelist goes berzerk
      x ../../testfiles/1/202.tst - freelist goes berzerk
    --duplicate=1 --use-cursors=1
      o ../../testfiles/1/100.tst - status mismatch
      o ../../testfiles/1/202.tst - status mismatch
      o ../../testfiles/1/220.tst - status mismatch




o create a new repository for hamsterdb-alien for all dependencies 
    in source and precompiled (static/non-debug - cppunit and berkeleydb)
    x linux64-le
    x linux32-le
    o cygwin32
    o win32
    o win64
    o wince-x86
    o ppc32-be

o extkey_cache: merge extkey_cache_remove() and blob_free()

o key_set_record does not always need to dirty the page (i.e. when
    appending duplicates) -> don't set page dirty in the caller

o cursor_overwrite/cursor_insert: 
    currently, the cursor_dupe_cache is deleted, but we don't have to
    also, in cursor_move there are some optimizations regarding the
    cache (search for TODO)

. new option HAM_DISABLE_AUTO_REORG -> adds empty chunks to freelist, but
    never uses freelist for allocation

. new option HAM_DISABLE_FREELIST -> completely disables the freelist (needs
    cmd line reorg-tool!)

o modify acceptance test for --duplicate=1 (berkdb and hamster)
    o new acceptance tests with many, many duplicates
    o --duplicate-position="before"|"after"|"first"|"last"(default)
        o add to monster.sh
        o add to valgrind.sh

o do benchmarking with 1mio inserts/reads

o release 0.4.7

. webpage
    o move to cakephp framework
    o frontpage: resize hamster picture; remove text -> more space below
        (for 3 colums: news, main features, articles/testimonials/link cloud)

. if only one duplicate is left, we could delete the duplicate table
    does this make sense?

. be more careful when uncoupling cursors - especially when inserting 
    or deleting items, uncoupling is often not necessary
    o definitely no need to uncouple if overwriting or adding a duplicate
    o unittests: 
        o create cursor -> must be NIL
        o insert item   -> must be NIL
        o move cursor to item
        o insert item2 < item -> cursor is uncoupled
        o move cursor to item
        o insert duplicate of item -> cursor is still coupled
        o insert item2 > item -> cursor is still coupled

. some parts of the code would be nicer if we had ham_mem_realloc
    and ham_mem_calloc
