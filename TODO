I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.1.7 ..................................................
x simplified btree SMOs
x improve recovery performance/scalability
x improve QuickCheck coverage
o delta updates

--------------

x Separate/Refactor SMOs: we want to run them separated from the actual
    operation in order to move them to the background sooner or later. Also,
    the erase SMOs are causing issues and have huge complexity but do not have
    many benefits.

x there'S a bug (already in 2.1.6)
    ./ham_bench --use-berkeleydb=true --reopen=true --key=binary --keysize=64 --pagesize=1024 --recsize=0 --bulk-erase --cache=104857600 --stop-ops=25000
    ./ham_bench --use-berkeleydb=true --reopen=true --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --cache=104857600 --stop-ops=25000
    ./ham_bench --use-berkeleydb=true --reopen=true --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --erase-pct=100 --cache=104857600 --stop-ops=25000
    ./ham_bench --use-berkeleydb=true --reopen=true --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --bulk-erase --cache=104857600 --stop-ops=25000

    x the overflow chain and state is not cleaned up properly if the
        new state is empty
    x needs more test

x delta updates (DU)
    x insert SMOs (splits) are applied when "going down"
        x make sure that all tests are still running
        x also check monster tests
    x erase SMOs are vastly simplified (only delete pages; no merges, no shifts)
        x leaf nodes can become empty
        x internal nodes must at least set ptr_down
        x when going down: only merge pages if they have the same parent and
            if they are both (nearly) empty
        -> in the end there's a btree skeleton with empty leafs and nearly-
            empty nodes; this can then be compact()ed offline or asynchronously
        x get rid of shifting and replace_key
        x BtreeImpl/Proxy: remove shifts, replace_key etc
        x also merge with the left sibling
        x check the monster tests

        x review BtreeImplDefault::requires_split
            -> causes segfaults because insert() assumes that there's enough
                space in the node. how to fix this?
                instead of 32 bytes, use...
                - fixed key length if keys are fixed length (but min. 16 bytes)
                - extkey-threshold if keys are variable length

    x test everything
        x run unittests tests
        x run recovery tests
        x run valgrind tests
        x run monster tests
        x run performance tests

    x is there a cheap way to visualize the tree?
        -> yes: http://www.graphviz.org/content/datastruct
        dot -Tpng -o test.png ~/test
        x embed into ham_db_check_integrity (add |flags| parameter)
        x check with multi-level (> 4) trees and bulk-erase
            -> only the lowest two levels should become empty

    x the erase algorithm only merges leaf nodes; if the tree has more than 2
        levels then it will not degenerate properly when all entries are
        deleted.
        x is this a problem? what if data is then re-inserted? -> seems
            to be fine, but needs more tests

x there's a bug (already in 2.1.6)
    ./ham_bench --key=binary --keysize=64 --pagesize=1024 --recsize=0 --bulk-erase --stop-ops=25000
    ./ham_bench --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --stop-ops=25000
    ./ham_bench --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --erase-pct=100 --stop-ops=25000
    ./ham_bench --reopen=true --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --bulk-erase --stop-ops=25000

    x the overflow chain and state is not cleaned up properly if the
        new state is empty
    x needs more test

    x --key=binary --keysize=64 --pagesize=1024 --recsize=0 --bulk-erase --distribution=descending --extkey-threshold=20 --recsize=0 --stop-ops=100000 --seed=1393420356
    x --key=binary --keysize=64 --pagesize=1024 --recsize=0 --bulk-erase --distribution=random --extkey-threshold=20 --recsize=0 --stop-ops=100000 --seed=1393420495

x another bug
    ./ham_bench --key=binary --keysize=64 --pagesize=1024 --recsize=0 --bulk-erase --stop-ops=25000 --seed=1393445737
    ./ham_bench --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --stop-ops=25000 --seed=1393445737
    ./ham_bench --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --erase-pct=100 --stop-ops=25000 --seed=1393445737
    (next line is repeated 3 times, 3rd time crashes)
    ./ham_bench --reopen=true --key=binary --keysize=64 --pagesize=1024 --recsize=0 --open --bulk-erase --stop-ops=25000 --seed=1393445762

x improve: ./ham_bench --recsize=1048576

x Improve transaction delta updates
    x improve transaction structures, decouple them
        x update to new rb-tree if available - nope
        x each TransactionUpdate should be a single allocation
        x Transaction should be a single-linked list
            head is the chronologically oldest txn
            tail is the chronologically newest txn
            append new transactions at tail
            flush them from head
        x temp. transactions should not be required for read-only ops
            x db::find
            x cursor::find
            x cursor::get_record_count
            x cursor::get_record_size
            x cursor::move
        x temp. transactions must not write to the journal
            x for insert/erase and all cursor functions
            x recovery: must accept updates w/ txn-id 0 (for temp. transactions)
            x add new test
        x more code cleanups
            x split LocalTransaction, RemoteTransaction?
            x txn_begin returns Transaction*; txn_commit and txn_abort ret. void
            x move logic from env to txn
            x check TODOs in txn_remote.cc
        x check_integrity should not require a |txn| parameter
        x improve splits of txn logic vs. btree logic, i.e. in
            x check_insert_conflicts et al; need a very clear separation of
                code, otherwise locking will be very tricky

    x do not flush committed transactions as long as their data consumption
        does not exceed a certain size, or the number of elements is not
        too high
        x log file must not be switched before the txn is flushed!
        x ham_env_flush and when closing: flush EVERYTHING
        x flush immediately if HAM_FLUSH_WHEN_COMMITTED
        x set threshold through an internal (global) variable
        x add unittest

        x clean up the code; make it less hackish
            x have a central authority for dealing with transactions and
                for flushing them ("TransactionManager"
                    -> "LocalTransactionManager" etc)
            x commits/aborts are also routed through the TransactionManager,
                which will do the logging (if reqd)
            x also manages the Transaction Id
            x TransactionManager keeps the threshold
            x also keep track of number of committed operations
            x ... and the consumed memory
            x check threshold for number of committed operations
            x check threshold for consumed memory (should be same as
                journal buffer size)
            x check with valgrind

        x expose the new flag
            x dotnet
            x java
            x ham_bench
            x erlang
        x add documentation to header file

        x add to monster test, valgrind test, performance test

    x check monster tests

    x combine operations of all flushed transactions in the same changeset
        -> we need to remove many asserts... :-/
        x when fetching a page: give a flag kReadOnly; then do not add
            the page to the changeset
        x make read-only operations fetch pages in read-only mode
            (btree_find, btree_check, BlobManager::read, others?)
        x then combine the operations
        x run recovery tests
        x add to monster tests and perftests (with and without immediate flush)

    x BtreeErase: remove ifdef

    x allow use of Transactions without journalling/recovery
        x implement
        x requires a test; just make sure that the journal is not created
        x needs documentation in header file
        x needs monster tests

    x how would a bulk update API look like??
        -> it would allocate memory for the user, and immediately add
            it to the txn-tree or append as a delta update.
        -> the user specifies whether transactions are bypassed or not
        -> the user specifies whether the data is already sorted or not
        -> the user specifies whether existing keys are overwritten
            or not

x Make cursor cleanup logic after erase less cheesy
    if a key is erased (remove_entry) then all cursors are uncoupled.
    Shortly afterwards, they're coupled again for the points_to() check.

x why is --enable-recovery twice as fast as --use-transactions?
    -> mostly related to malloc/free in the rb-tree
    x what can we do to reduce the allocations? a typical insert requires
        three allocations (we could get down to two):
        - node
        - key
        - op + record (already combined)
        -> rb.h requires a single key; how can we re-structure? 
        --> allocate a single structure (same as DeltaUpdate), and let the
            TransactionTree point to this update. The TransactionNode
            can manage a list of DeltaUpdates for the same key. As soon as
            one DeltaUpdate is removed, the key will be provided by the
            next update.
            Later, the DUs can then be attached to the Btree node.
    x the TransactionOperation structure also stores the key (one single
        allocation!)
    x the TransactionNode no longer stores the key but uses the one from a
        connected TransactionOperation

x QuickCheck
    x extend existing test with overwrite and duplicates
    x document the various tests
    x new test: create a database with generated configuration; use
        insert/erase/find/close/open; use very low pagesize with large
        key size, if possible; also use multi-page blobs

o webpage copyright is still 2013

o improve the release process
    o improve/add integration of static code analysis to release process
        o oclint: create helper file in bin/
            oclint $FILE -- -I ../include/ -D__STDC_CONSTANT_MACROS -D__STDC_LIMIT_MACROS -I/usr/include -I/usr/local/include -c
            filter out P1 problems, report P2, ignore P3
        o coverity
    o packaging currently creates the documentation tarball with the
        full directory
    o run both as part of the release process
    o move the critical stages up front, and the non critical ones towards
        the end (critical: packaging, unittests, tools, samples, valgrind)

o extend ham_bench documentation for --flush-txn-immediately
o extend ham_bench documentation for --disable-recovery

x web-page requires updates
    x deployed html differs from git-repository
    x download/sources: add erlang, remove 1.x
    x www1-repository and hamsterdb-www should be identical
        x updates for 2.1.6 are missing
        x samples fehlen
        x doku fehlt
        x download-dateien fehlen
    x www1
        x www1 and www2 are already combined in a single remote target
        x where to host static files? - hetzner server
        x clean up 'dl' directory
    o 'htdocs' should push to staging.hamsterdb.com
        o if everything works then also to hamsterdb.com
        o then use a different branch for staging
    o merge all repositories (host on github, keep remote branch)
        hamsterdb-www is the main repository, master pushes to staging
        'stable' pushes to hamsterdb.com, www1, www2

. collect file format updates
    o PageManager state: also store m_last_blob_page_id persistently
        (try not to break backwards compatibility, though)
    o reserve CRC32 for each page

. QuickCheck
    o create a new property for testing duplicates; the key is always the
        same. The number of duplicate keys is tracked and periodically checked
        with the API. A cursor can be used to remove a specific duplicate,
        or to fetch a specific duplicate.

----------------------------------

o delta updates managed in the BtreeNode
    the catch is that the actual insert operation is separated from
    the user's insert operation. The page will be logged and flushed,
    but the update is not performed.
    -> make sure the page is not "dirty" if the delta update is appended,
    -> ... only when it's merged
    -> then how will it be added to the changeset?
    -> is a page dirty if it has DUs?

    o perform a benchmark/profiling: random inserts vs. ascending inserts;
        the difference should be caused by memcopy/memmove (is it?)
        x PAX
            -> absolutely worth the effort, about 60% are spent in memmove
        o Default

    o if the deltas are merged then the page must be added to the
        changeset; invest more time in thinking about recovery and
        make this water proof!!

    o need a flag to disable DeltaUpdates
    o rename TransactionOperation to DeltaUpdate, decouple code from txn
    o totally transparent to the caller, handled in the proxy
    o merge them when reading and flushing
    o requires_split() takes delta updates into account
    o only add deltas to leaf nodes; internal nodes have too many read
        operations and would anyway require immediate flushes
    o DeltaUpdate objects from a txn-flush should immediately go down to
        the node
    o make the merge algorithm more efficient
        o sort deltas by key
        o first execute all 'erase' either against other deltas or against
            the node
        o then merge the remaining inserts
        o this needs lots of tests

o Start with the PRO version
    x API to get licensee information (can use ham_get_license())
        x also for wrapper APIs
    x update all file header comments; describe copyright, licensing options
        and point to the COPYING file.
    o change file header for include files, too
    o fork a closed repository
        x replace COPYING file
        o PRO is able to open MIT files, but not vice versa
        o change ham_get_license()
            o needs a test
        o enable AES encryption (also for the journal)
            o journal header: store flags ("Encryption enabled")
            o journal entries: requires padding??
            o apply encryption on the lowest level (whereas compression
                will be on entry-level)
            o MIT should return "HAM_NOT_IMPLEMENTED"
            o needs unittests, perftests and monster tests
        o enable zlib/snappy/others compression (for records and the journal)
            o compressor.h
            o compressor_snappy.h
            o compressor_zlib.h
            o compressor_lzf.h
                http://oldhome.schmorp.de/marc/liblzf.html - BSD license
                o add to alien/lzf
            o compressor_lzop.h
                http://www.oberhumer.com/opensource/lzo/ - GPL license (+ comm)
            o compressor_factory.h
            o compressor_factory.cc
            o unittest for compressors, also acts as performance test

            o new persistent parameter HAM_PARAM_ENABLE_COMPRESSION
                        = Record | Jrnal
            o new parameter HAM_PARAM_SET_RECORD_COMPRESSION_LIBRARY = library
            o new parameter HAM_PARAM_SET_RECORD_COMPRESSION_LEVEL
            o new parameter HAM_PARAM_SET_JOURNAL_COMPRESSION_LIBRARY = library
            o new parameter HAM_PARAM_SET_JOURNAL_COMPRESSION_LEVEL

            o MIT should return "HAM_NOT_IMPLEMENTED"
            o do not allow partial updates for compressed records
                o unittests
            o add to java, .NET, erlang
            o implement for the journal
            o implement for the records

            o for the actual defaults: run tests, also with string input (from
                    the word list - already supported in ham_bench)
                o then pick the fastest option as the default

            o needs unittests, perftests and monster tests
        o replace GPL with MIT license (in hamsterdb, then merge)

    o work over the license agreement

    o 30 day evaluation library
        o insert 30day trial check macros at various positions
            o dynamically generate those macros
        o needs different licensing output in the tools
        o print to stdout in ham_env_create, ham_env_open
        o evaluate source obfuscators
        o create one source/win32 package starting at each week, running
                4 weeks
            o requires a build tool for unix and win32

    o new release process for the commercial build
        o also create the 30day trial packages

    o admin webpage
        o database is in openshift
        o admin webpage is secured by htaccess (has a non-standard path)
        o has a dashboard
            o number of customers
            o number of customers which will expire in less than 1 month
            o number of trial customers
            o number of trial customers which will expire in less than 1 week
        o has a table with the releases
        o has a table with the customers and their license status
                (trial from-to, customer from-to)
        o has a table with the links/guids for the customers
        o can list, add, edit, remove releases
        o can list, add, edit, remove customers
        o can create a new trial customer
        o can create a new (non-trial) customer
        o can upgrade a customer
        o can list trial customers which expire soon
        o can list real customers which expire soon
        o has an api to verify the GUIDs
        o daily backup of the database to the other server

        o need a second installation for testing!

    o webpage
        o overview, catch lines
        o customers
        o feature matrix MIT vs. closed source
        o integrate blog, use disqus comments
        o can download commercial files using the admin-API to verify the
            login
        o daily backups of the database

    o expired 30day licenses will be removed automatically from the server
        o and from the backup server
        o and from the database

    o what are the minimum features required for the first release?
        - (evaluation licenses)
        - lightweight compression for the journal
        - lightweight compression for records
        - AES encryption for journal and file
        - hot backups
      optional:
        - bulk updates
        - prefix compression for strings (requires delta updates)
        - lightweight compression for binary keys (requires delta updates)
        - SIMD for searches

o compression for variable-length keys
    - if key is appended at the end: just write the delta
    - otherwise things get tricky, because the keys to the "right" of the
        inserted key have to be re-compressed; and in this case it is
        possible that the space will overflow
    - when re-compressing: key size might grow or shrink, which means that
        the keys "to the right" should be rearranged as well
    ===> it really makes sense to have delta updates and bulk inserts first!!
    - try to save a full key every Nth bytes, to avoid extensive re-compressing

- "hola" - olap functions that operate directly on the btree data

x prefetch cache lines when searching -> no effect
- encrypt the log and the environment file with AES -> PRO
- compress the log with snappy, zlib, others -> PRO
- compress the records with snappy, zlib, others -> PRO
- hot backups (vacuumizes to a different file) -> PRO
    - copies the database file
    - if compaction is enabled: copies keys w/ iterator
        (later: performs bulk updates)
    - then applies all committed transactions to the other file
- bulk updates -> PRO
    - require delta updates
    - give users API to allocate memory for keys/records
    - if user says that data is already sorted then do not re-sort
    - add those delta updates to the txn-trees or to the stream
- linear SIMD search -> PRO (see topic/linear)
- first PRO release
    o webpage updates, PR, mailings

- compression -> PRO
- cache-oblivious page distribution?
    http://supertech.csail.mit.edu/cacheObliviousBTree.html
- bloom filter -> PRO
- concurrency -> PRO
- operations on compressed data (COUNT(), MIN(), MAX(), ...)?
- C-Store style projections? (requires complex schema types)
    - introduce a new library with complex schema types, projections
    - analytic functions
        count, min, max, sum, product, average, ln, sqrt, exp, round, trunc,
        date/time functions and interval functions
    - select(predicate_t, column_descriptor_t, select_state_t)
             \- an AST of functors
                          \- existing columns or generated columns (i.e. sum())
                                                \- keeps track of offset, count
    - erase(predicate_t)
    - explain(predicate_t, column_descriptor_t)

o PRO: btree can compress keys
    x get rid of the whole minkey/maxkey handling because these numbers
        are irrelevant with compression
    o try to reduce the changes to a new KeyProxy object
    o prefix-compression for strings
        o each 2kb have a full string (indexed by skiplist)
    o delta-compression for numeric data types (>= 32bit)
        (can this be combined with a bitmap compression? the deltas are
        compressed in a bit-stream? but then we end up with variable
        length encodings...)
    o lightweight compression for keys
        http://oldhome.schmorp.de/marc/liblzf.html
        http://lzop.org
    o record compression for blobs (lzop.org? snappy?)
        better postpone this and compress all pages in the lss
    o do we need delta updates for efficient inserts? - i think not yet...

o PRO: use SIMD for fixed-length scans and __builtin_prefetch
    -> see topic/linear for a partial implementation

o the bucket for concurrency TODOs
    o reduce the linked lists - they're hard to be updated with concurrent
        operations
        o page
        o transaction and dependent objects
        o ...

    . come up with a list of all functions, define which locking operation
        is required; then review the code and make sure this will work
    . come up with a list of functions for which concurrency makes most sense
        - parallel lookups (using the same memory arena)
        - flushing transactions asynchronously
        - purging caches asynchronously
        - async. merging of delta updates
        - have concurrent lookups/find

    o separate SMOs from the actual operation (#2)
        -> check the literature
        http://pdf.aminer.org/000/409/763/b_trees_with_relaxed_balance.pdf
        o move SMO operations to "the janitor" (btree_smo.h)

o use cache-oblivious b-tree layout
    o see roadmap document for more information
    o run a performance test/prototype if this is worth the effort
        o allocate a fixed number of pages (20) for the index
        o PageManager: when allocating a new page then use the distribution
            function to fetch a page from the reserved storage
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

o btree_impl_default::set_record: if the duplicate is LAST of the last key
    in the node then simply append the record and increase next_offset

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o flush transactions in background (when the btree is concurrent)

o Improve leaf pages caching
    Store start/end key of each leaf page in a separate lookup table in order
    to avoid btree traversals. This could be part of the hinter.
  - one such cache per database
  - should work for insert/find/erase

o allow transactions w/o journal

o allow transactions w/o recovery

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o when flushing the Changeset: batch ALL changes for the WHOLE transaction,
    then flush all of them together. This way we can "merge" multiple changes
    for the same page.
    Also review the whole flush process - when not to log etc.
    - only 1 page affected: no need to log it because it is idempotent
    - freelist pages are always idempotent
    - more than 1 index page? not idempotent (most likely)
    - more than 1 blob page? not idempotent (maybe)
    o define a few benchmarks
    o be careful: if N operations are modifying the same changelog, and
        then #N+1 aborts then the aborting operation must NOT clear the
        changelog!

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o changeset: instead of simply adding pages to the changeset, the caller
    could already specify whether this page needs logging or not;
    i.e. after freelist rewrite, the blob pages do not need logging if a
    blob is deleted  

o is there a way to group all changeset flushes of a Transaction into one
    changeset, and batch-commit multiple commits? that way we would avoid the
    frequent syncs and performance would be improved
    o would have to remove all of assert(changeset.is_empty())
    o but we can use that assert prior to txn_begin

o flush in background (asynchronously)
    o need new flag file HAM_DISABLE_ASYNCHRONOUS_FLUSH
    o if in-memory database: disable async flush
    o if transactions are disabled: disable async flush
    o if enabled: create background thread, wait for signal
    o ham_env_flush: if txn are enabled then try to flush them to disk
    o how to deal with an error in the background thread???
        o store in Environment, then return in every exported function
    o default: async flush is OFF!

    o extend monster tests
        o with async flush
        o without async flush
        o extend/run performance test
        o run monster tests

    o documentation
        o tutorial
        o faq

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

. new flag for transactions: HAM_TXN_WILL_COMMIT
    if this flag is set, then write all records directly to the file, not
    to the log. the log will only contain the rid.
    -> or: make this the default; call the new flag HAM_TXN_MAYBE_WILL_ABORT
    o in case of an abort: move the record to the freelist
    -> this affects all temporary ham_insert-transactions
    (not sure if this should get high priority)

. if memory consumption in the txn-tree is too high: flush records to disk
    (not sure if this should get high priority)

o ham_get_count: could be atomically updated with every journal entry

=======
>>>>>>> Updated TODO
